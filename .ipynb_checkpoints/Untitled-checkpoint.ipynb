{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "fff1a470-5a26-4c06-aa12-74a764d597cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import re\n",
    "import time\n",
    "import random\n",
    "from datetime import datetime\n",
    "from typing import List, Dict, Any, Optional, Tuple, Set\n",
    "from collections import Counter, defaultdict\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from dotenv import load_dotenv\n",
    "from langchain_community.document_loaders import PyPDFLoader\n",
    "from langchain_openai import OpenAIEmbeddings\n",
    "from neo4j import GraphDatabase, Driver\n",
    "from openai import OpenAI\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "af59dd71-62d7-4bc4-9996-a1cab5f38653",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_config_and_initialize():\n",
    "    \"\"\"환경 변수 로드 및 주요 객체 초기화\"\"\"\n",
    "    # .env 파일 로드\n",
    "    load_dotenv() \n",
    "\n",
    "    config = {\n",
    "        \"neo4j_uri\": os.getenv(\"NEO4J_URI\"),\n",
    "        \"neo4j_username\": os.getenv(\"NEO4J_USERNAME\"),\n",
    "        \"neo4j_password\": os.getenv(\"NEO4J_PASSWORD\"),\n",
    "        \"openai_api_key\": os.getenv(\"OPENAI_API_KEY\"),\n",
    "        \"embedding_model_name\": 'text-embedding-3-small',\n",
    "        \"embedding_dimension\": 1536,\n",
    "        \"pdf_path\": './dataset/criminal-law.pdf',\n",
    "        \"precedent_dir\": './dataset/precedent_label/',\n",
    "        \"test_csv_path\": './dataset/Criminal-Law-test.csv',\n",
    "        \"results_dir\": \"results\",\n",
    "        \"llm_model\": \"gpt-4o-mini\",\n",
    "    }\n",
    "\n",
    "    # 결과 디렉토리 생성\n",
    "    os.makedirs(config[\"results_dir\"], exist_ok=True)\n",
    "\n",
    "    # OpenAI API 키 확인\n",
    "    if not config[\"openai_api_key\"]:\n",
    "        print(\"Warning: OpenAI API Key not set. Please set the OPENAI_API_KEY environment variable.\")\n",
    "        return None, None, None\n",
    "\n",
    "    print(\"OpenAI API Key loaded.\")\n",
    "\n",
    "    # Embedding 모델 설정\n",
    "    try:\n",
    "        embedding_model = OpenAIEmbeddings(\n",
    "            model=config[\"embedding_model_name\"],\n",
    "            api_key=config[\"openai_api_key\"]\n",
    "        )\n",
    "        print(f\"Embedding model '{config['embedding_model_name']}' initialized.\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error initializing embedding model: {e}\")\n",
    "        embedding_model = None\n",
    "\n",
    "    # OpenAI 클라이언트 초기화\n",
    "    try:\n",
    "        openai_client = OpenAI(api_key=config[\"openai_api_key\"])\n",
    "        print(\"OpenAI client initialized.\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error initializing OpenAI client: {e}\")\n",
    "        openai_client = None\n",
    "\n",
    "    print(\"Configuration loaded and objects initialized.\")\n",
    "    return config, embedding_model, openai_client"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "52acbaec-c7b5-4d22-a7b6-23499d6a0988",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_articles_from_pdf(pdf_path: str) -> Dict[str, str]:\n",
    "    \"\"\"PDF에서 법 조항 텍스트를 로드하고 추출\"\"\"\n",
    "    print(f\"Loading articles from PDF: {pdf_path}\")\n",
    "    if not os.path.exists(pdf_path):\n",
    "        print(f\"Error: PDF file not found at {pdf_path}\")\n",
    "        return {}\n",
    "\n",
    "    try:\n",
    "        loader = PyPDFLoader(pdf_path)\n",
    "        pages = loader.load()\n",
    "        full_text = \"\\n\".join(page.page_content for page in pages)\n",
    "        print(f\"Loaded {len(pages)} pages from PDF.\")\n",
    "\n",
    "        # 조항 패턴 수정 (괄호 안 내용 포함, 공백 유연하게 처리)\n",
    "        article_pattern = r'(제\\s*\\d+\\s*조(?:의\\s*\\d+)?(?:\\s*\\(.*?\\))?)'\n",
    "        matches = list(re.finditer(article_pattern, full_text))\n",
    "        print(f\"Found {len(matches)} potential article markers.\")\n",
    "\n",
    "        articles = {}\n",
    "        for i in range(len(matches)):\n",
    "            current_match = matches[i]\n",
    "            # 조항 ID 정규화 (공백 제거)\n",
    "            current_article_id = re.sub(r'\\s+', '', current_match.group(1)).strip()\n",
    "\n",
    "            start_pos = current_match.start()\n",
    "            end_pos = matches[i+1].start() if i < len(matches)-1 else len(full_text)\n",
    "            article_text = full_text[start_pos:end_pos].strip()\n",
    "\n",
    "            # 내용이 너무 짧으면 건너뛰기 (예: 목차 등에서 잘못 추출된 경우)\n",
    "            if len(article_text) > 50: # 최소 길이 기준 설정\n",
    "                 articles[current_article_id] = article_text\n",
    "\n",
    "        print(f\"Processed {len(articles)} articles from PDF.\")\n",
    "        # 예시 출력 (3개만)\n",
    "        if articles:\n",
    "            article_ids = list(articles.keys())\n",
    "            print(\"\\n--- First 3 Articles (Preview) ---\")\n",
    "            for article_id in article_ids[:3]:\n",
    "                print(f\"\\n--- Article: {article_id} ---\")\n",
    "                print(articles[article_id][:150] + \"...\")\n",
    "        return articles\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred while loading/processing the PDF: {e}\")\n",
    "        return {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e335a4b3-211d-4c48-b2d2-7e6371976271",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_precedents_from_json(precedent_dir: str, sample_size: Optional[int] = 1000) -> List[Dict[str, Any]]:\n",
    "    \"\"\"JSON 파일에서 판례 정보를 로드하고 정제 (관계 추출 개선)\"\"\"\n",
    "    print(f\"Loading precedents from directory: {precedent_dir}\")\n",
    "    if not os.path.isdir(precedent_dir):\n",
    "        print(f\"Error: Precedent directory not found at {precedent_dir}\")\n",
    "        return []\n",
    "\n",
    "    precedents = []\n",
    "    # 법조항 패턴 (공백 유연성 증가, 더 다양한 형태 매칭)\n",
    "    rule_pattern = re.compile(r'제\\s*\\d+\\s*조(?:의\\s*\\d+)?(?:\\s*\\(.*?\\))?')\n",
    "    files_processed = 0\n",
    "    files_skipped = 0\n",
    "    keyword_stats = Counter()  # 키워드 통계\n",
    "    rule_refs_stats = Counter()  # 참조 법조항 통계\n",
    "\n",
    "    json_files = [f for f in os.listdir(precedent_dir) if f.endswith(\".json\")]\n",
    "    print(f\"Found {len(json_files)} JSON files.\")\n",
    "\n",
    "    for filename in tqdm(json_files, desc=\"Loading precedents\"):\n",
    "        filepath = os.path.join(precedent_dir, filename)\n",
    "        try:\n",
    "            with open(filepath, 'r', encoding='utf-8') as f:\n",
    "                data = json.load(f)\n",
    "                info = data.get(\"info\", {})\n",
    "                summary_list = data.get(\"Summary\", [])\n",
    "                keyword_list = data.get(\"keyword_tagg\", [])\n",
    "                ref_info = data.get(\"Reference_info\", {})\n",
    "\n",
    "                precedent_info = {\n",
    "                    \"case_id\": info.get(\"caseNoID\", filename.replace(\".json\", \"\")),\n",
    "                    \"case_name\": info.get(\"caseNm\", \"Unnamed Case\"),\n",
    "                    \"judgment_summary\": data.get(\"jdgmn\", \"\"),\n",
    "                    \"full_summary\": \" \".join([s.get(\"summ_contxt\", \"\") for s in summary_list]).strip(),\n",
    "                    \"keywords\": [kw.get(\"keyword\") for kw in keyword_list if kw.get(\"keyword\")],\n",
    "                    \"referenced_rules_raw\": ref_info.get(\"reference_rules\", \"\"),\n",
    "                    \"referenced_cases_raw\": ref_info.get(\"reference_court_case\", \"\"),\n",
    "                }\n",
    "\n",
    "                # === 개선됨: 키워드가 없는 경우 간단한 키워드 추출 ===\n",
    "                if not precedent_info[\"keywords\"] and precedent_info[\"full_summary\"]:\n",
    "                    # 간단한 키워드 추출 (명사 빈도 기반)\n",
    "                    extracted_keywords = extract_simple_keywords(precedent_info[\"full_summary\"], top_n=5)\n",
    "                    precedent_info[\"keywords\"] = extracted_keywords\n",
    "                    precedent_info[\"keywords_extracted\"] = True  # 자동 추출 플래그\n",
    "\n",
    "                # === 개선됨: 참조 법조항 추출 강화 ===\n",
    "                raw_rules = precedent_info[\"referenced_rules_raw\"].split(',') if precedent_info[\"referenced_rules_raw\"] else []\n",
    "                all_text = precedent_info[\"full_summary\"] + \" \" + precedent_info[\"judgment_summary\"]\n",
    "                \n",
    "                # 참조 법조항 정제 - 모든 가능한 소스에서 추출\n",
    "                cleaned_rules = set()\n",
    "                # 1. 명시적 참조 목록에서 추출\n",
    "                for rule in raw_rules:\n",
    "                    matches = rule_pattern.findall(rule.strip())\n",
    "                    for match in matches:\n",
    "                        clean_match = re.sub(r'\\s+', '', match)  # 공백 제거\n",
    "                        cleaned_rules.add(clean_match)\n",
    "                \n",
    "                # 2. 전체 텍스트에서 법조항 패턴 추출 (명시적 참조가 없거나 불완전한 경우)\n",
    "                if len(cleaned_rules) < 2 and all_text:  # 참조 법조항이 없거나 적은 경우\n",
    "                    text_matches = rule_pattern.findall(all_text)\n",
    "                    for match in text_matches:\n",
    "                        clean_match = re.sub(r'\\s+', '', match)\n",
    "                        cleaned_rules.add(clean_match)\n",
    "                \n",
    "                precedent_info[\"referenced_rules\"] = list(cleaned_rules)\n",
    "\n",
    "                # === 개선됨: 참조 판례 정제 및 추가 정보 ===\n",
    "                raw_cases = precedent_info[\"referenced_cases_raw\"].split(',') if precedent_info[\"referenced_cases_raw\"] else []\n",
    "                referenced_cases = []\n",
    "                for case in raw_cases:\n",
    "                    case_clean = case.strip()\n",
    "                    if case_clean:\n",
    "                        # 판례 ID 추출 시도 (예: \"대법원 2020다12345\" -> \"2020다12345\")\n",
    "                        case_id_match = re.search(r'\\d+\\s*[가-힣]+\\s*\\d+', case_clean)\n",
    "                        if case_id_match:\n",
    "                            case_id = re.sub(r'\\s+', '', case_id_match.group(0))\n",
    "                            referenced_cases.append({\n",
    "                                \"full_reference\": case_clean,\n",
    "                                \"case_id\": case_id\n",
    "                            })\n",
    "                        else:\n",
    "                            referenced_cases.append({\n",
    "                                \"full_reference\": case_clean,\n",
    "                                \"case_id\": None\n",
    "                            })\n",
    "                \n",
    "                precedent_info[\"referenced_cases\"] = referenced_cases\n",
    "\n",
    "                # 임베딩할 텍스트 준비 (full_summary 우선, 없으면 judgment_summary)\n",
    "                precedent_info[\"text_for_embedding\"] = precedent_info[\"full_summary\"] or precedent_info[\"judgment_summary\"]\n",
    "\n",
    "                # 유효한 데이터만 추가 (임베딩할 텍스트가 있어야 함)\n",
    "                if precedent_info[\"text_for_embedding\"]:\n",
    "                    precedents.append(precedent_info)\n",
    "                    files_processed += 1\n",
    "                    \n",
    "                    # 통계 수집\n",
    "                    keyword_stats.update(precedent_info[\"keywords\"])\n",
    "                    rule_refs_stats.update(precedent_info[\"referenced_rules\"])\n",
    "                else:\n",
    "                    files_skipped += 1\n",
    "\n",
    "        except json.JSONDecodeError:\n",
    "            print(f\"Warning: Could not decode JSON from {filename}\")\n",
    "            files_skipped += 1\n",
    "        except Exception as e:\n",
    "            print(f\"Error processing {filename}: {e}\")\n",
    "            files_skipped += 1\n",
    "\n",
    "    # === 개선됨: 데이터 로드 후 통계 출력 ===\n",
    "    print(f\"Loaded {len(precedents)} valid precedents. Skipped {files_skipped} files.\")\n",
    "    print(f\"Total unique keywords found: {len(keyword_stats)}\")\n",
    "    print(f\"Total unique referenced rules found: {len(rule_refs_stats)}\")\n",
    "    print(f\"Top 10 most common keywords: {keyword_stats.most_common(10)}\")\n",
    "    print(f\"Top 10 most referenced rules: {rule_refs_stats.most_common(10)}\")\n",
    "    \n",
    "    # 키워드가 없거나 참조 법조항이 없는 판례 수 출력\n",
    "    no_keywords = sum(1 for p in precedents if not p[\"keywords\"])\n",
    "    no_rules = sum(1 for p in precedents if not p[\"referenced_rules\"])\n",
    "    print(f\"Precedents without keywords: {no_keywords} ({no_keywords/len(precedents):.1%})\")\n",
    "    print(f\"Precedents without referenced rules: {no_rules} ({no_rules/len(precedents):.1%})\")\n",
    "\n",
    "    # 샘플링\n",
    "    if sample_size is not None and len(precedents) > sample_size:\n",
    "        print(f\"Sampling {sample_size} precedents from {len(precedents)}...\")\n",
    "        random.seed(42) # 재현성을 위한 시드 고정\n",
    "        precedents = random.sample(precedents, sample_size)\n",
    "        print(f\"Selected {len(precedents)} precedents after sampling.\")\n",
    "\n",
    "    return precedents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7b15e6be-4a9f-4d78-81cf-e4ccbeba213c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_simple_keywords(text: str, top_n: int = 5) -> List[str]:\n",
    "    \"\"\"텍스트에서 간단한 키워드 추출 (빈도 기반)\"\"\"\n",
    "    # 한국어 불용어 목록\n",
    "    stopwords = set([\n",
    "        \"의\", \"가\", \"이\", \"은\", \"들\", \"는\", \"좀\", \"잘\", \"걍\", \"과\", \"도\", \"를\", \"으로\", \"자\", \"에\", \"와\", \"한\", \"하다\",\n",
    "        \"것\", \"그\", \"저\", \"수\", \"때\", \"등\", \"및\", \"제\", \"조\", \"항\", \"관련\", \"대한\", \"대해\", \"위한\", \"있는\", \"하는\",\n",
    "        \"그리고\", \"그러나\", \"그래서\", \"하지만\", \"또는\", \"다른\", \"모든\", \"어떤\", \"누구\", \"무엇\", \"언제\", \"어디서\", \"어떻게\", \"왜\",\n",
    "        \"입니다\", \"습니다\", \"합니다\", \"에서\", \"에게\", \"부터\", \"까지\", \"보다\", \"만\", \"같이\", \"처럼\", \"따라\", \"통해\"\n",
    "    ])\n",
    "    \n",
    "    # 2글자 이상 한글 단어 추출\n",
    "    words = re.findall(r'[가-힣]{2,}', text)\n",
    "    words = [word for word in words if word not in stopwords]\n",
    "    \n",
    "    # 빈도수 계산 및 상위 키워드 추출\n",
    "    counter = Counter(words)\n",
    "    top_keywords = [word for word, _ in counter.most_common(top_n)]\n",
    "    \n",
    "    return top_keywords\n",
    "\n",
    "def connect_neo4j(uri: str, auth: tuple) -> Optional[Driver]:\n",
    "    \"\"\"Neo4j 데이터베이스에 연결 (클라우드 Neo4j 연결 지원)\"\"\"\n",
    "    print(f\"Attempting to connect to Neo4j at {uri}...\")\n",
    "    try:\n",
    "        # 중요: URI 스키마 확인하여 적절한 설정 사용\n",
    "        is_secured_protocol = any(prefix in uri for prefix in ['+s', '+ssc'])\n",
    "        \n",
    "        # 보안 URI 스키마에 따라 설정 구성\n",
    "        if is_secured_protocol:\n",
    "            # neo4j+s://, bolt+s:// 등은 이미 암호화 설정이 포함됨\n",
    "            driver = GraphDatabase.driver(\n",
    "                uri, \n",
    "                auth=auth,\n",
    "                max_connection_lifetime=3600,\n",
    "                max_connection_pool_size=50,\n",
    "                connection_acquisition_timeout=60\n",
    "                # encrypted와 trust 설정을 제거함\n",
    "            )\n",
    "        else:\n",
    "            # 비보안 URI (neo4j://, bolt://)는 암호화 설정 필요\n",
    "            driver = GraphDatabase.driver(\n",
    "                uri, \n",
    "                auth=auth,\n",
    "                max_connection_lifetime=3600,\n",
    "                max_connection_pool_size=50,\n",
    "                connection_acquisition_timeout=60,\n",
    "                encrypted=True,\n",
    "                trust=\"TRUST_SYSTEM_CA_SIGNED_CERTIFICATES\"\n",
    "            )\n",
    "        \n",
    "        # 연결 테스트\n",
    "        with driver.session(database=\"neo4j\") as session:\n",
    "            result = session.run(\"RETURN 1 as n\")\n",
    "            record = result.single()\n",
    "            if record and record[\"n\"] == 1:\n",
    "                print(\"Successfully connected to Neo4j database.\")\n",
    "            else:\n",
    "                print(\"Warning: Connection established but verification failed.\")\n",
    "        \n",
    "        return driver\n",
    "    except Exception as e:\n",
    "        print(f\"Failed to connect to Neo4j: {e}\")\n",
    "        print(\"Please ensure Neo4j is running and the connection details are correct.\")\n",
    "        \n",
    "        # 디버깅 정보\n",
    "        print(f\"\\nConnection Details for Debugging:\")\n",
    "        print(f\"URI: {uri}\")\n",
    "        print(f\"Username provided: {bool(auth[0])}\")\n",
    "        print(f\"Password provided: {bool(auth[1])}\")\n",
    "        \n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c928b3f3-e313-4aca-b275-48538de02352",
   "metadata": {},
   "outputs": [],
   "source": [
    "def close_neo4j(driver: Optional[Driver]):\n",
    "    \"\"\"Neo4j 드라이버 연결 종료\"\"\"\n",
    "    if driver:\n",
    "        driver.close()\n",
    "        print(\"Neo4j driver connection closed.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b3f411df-f38d-45b7-be26-c9bef25e9cf4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def setup_neo4j_constraints_and_indexes(driver: Driver, dimension: int):\n",
    "    \"\"\"Neo4j 제약조건 및 벡터 인덱스 설정 (클라우드 Neo4j 호환성 개선)\"\"\"\n",
    "    print(\"Setting up Neo4j constraints and indexes...\")\n",
    "    try:\n",
    "        with driver.session(database=\"neo4j\") as session:\n",
    "            # === 개선됨: 제약조건 (클라우드 Neo4j 호환) ===\n",
    "            constraints = [\n",
    "                \"CREATE CONSTRAINT article_id IF NOT EXISTS FOR (a:Article) REQUIRE a.id IS UNIQUE\",\n",
    "                \"CREATE CONSTRAINT precedent_id IF NOT EXISTS FOR (p:Precedent) REQUIRE p.id IS UNIQUE\",\n",
    "                \"CREATE CONSTRAINT keyword_text IF NOT EXISTS FOR (k:Keyword) REQUIRE k.text IS UNIQUE\"\n",
    "            ]\n",
    "            \n",
    "            for constraint in constraints:\n",
    "                try:\n",
    "                    session.run(constraint)\n",
    "                except Exception as e:\n",
    "                    print(f\"Warning: Could not create constraint ({constraint}): {e}\")\n",
    "            \n",
    "            print(\"Constraints created or verified.\")\n",
    "\n",
    "            # === 개선됨: 일반 인덱스 (클라우드 Neo4j 호환) ===\n",
    "            indexes = [\n",
    "                \"CREATE INDEX keyword_text_idx IF NOT EXISTS FOR (k:Keyword) ON (k.text)\",\n",
    "                \"CREATE INDEX article_id_idx IF NOT EXISTS FOR (a:Article) ON (a.id)\",\n",
    "                \"CREATE INDEX precedent_id_idx IF NOT EXISTS FOR (p:Precedent) ON (p.id)\"\n",
    "            ]\n",
    "            \n",
    "            for index in indexes:\n",
    "                try:\n",
    "                    session.run(index)\n",
    "                except Exception as e:\n",
    "                    print(f\"Warning: Could not create index ({index}): {e}\")\n",
    "\n",
    "            # 벡터 인덱스 - 클라우드 Neo4j에 맞게 수정\n",
    "            index_commands = [\n",
    "                (f\"CREATE VECTOR INDEX article_embedding IF NOT EXISTS \"\n",
    "                 f\"FOR (a:Article) ON (a.embedding) \"\n",
    "                 f\"OPTIONS {{indexConfig: {{`vector.dimensions`: {dimension}, `vector.similarity_function`: 'cosine'}}}}\"),\n",
    "                (f\"CREATE VECTOR INDEX precedent_embedding IF NOT EXISTS \"\n",
    "                 f\"FOR (p:Precedent) ON (p.embedding) \"\n",
    "                 f\"OPTIONS {{indexConfig: {{`vector.dimensions`: {dimension}, `vector.similarity_function`: 'cosine'}}}}\")\n",
    "            ]\n",
    "            for command in index_commands:\n",
    "                try:\n",
    "                    session.run(command)\n",
    "                    index_name = command.split(\" \")[3] # 간단히 인덱스 이름 추출\n",
    "                    print(f\"Vector index '{index_name}' created or verified.\")\n",
    "                except Exception as e:\n",
    "                    # 벡터 인덱스 생성 실패는 경고로 처리\n",
    "                    print(f\"Warning: Could not create or verify vector index: {e}\")\n",
    "                    print(\"Vector search functionality might not be available.\")\n",
    "\n",
    "            # 인덱스 활성화 대기 (클라우드 환경에 맞게 타임아웃 조정)\n",
    "            print(\"Waiting for indexes to come online (up to 30 seconds)...\")\n",
    "            try:\n",
    "                session.run(\"CALL db.awaitIndexes(30000)\") # 30초 타임아웃\n",
    "                print(\"Indexes are online.\")\n",
    "            except Exception as e:\n",
    "                 print(f\"Warning: Could not explicitly wait for indexes: {e}\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred during Neo4j setup: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "84b08b9b-073d-4d6c-96d0-31463f96d509",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_graph_nodes_and_relationships(driver: Driver, articles: Dict[str, str], precedents: List[Dict[str, Any]], embed_model):\n",
    "    \"\"\"법 조항, 판례, 키워드 노드 및 관계를 배치 처리로 생성 (관계 생성 확실히)\"\"\"\n",
    "    if not driver:\n",
    "        print(\"Neo4j driver not available. Skipping graph creation.\")\n",
    "        return\n",
    "    if not embed_model:\n",
    "        print(\"Embedding model not available. Skipping graph creation.\")\n",
    "        return\n",
    "    if not articles and not precedents:\n",
    "        print(\"No articles or precedents data provided. Skipping graph creation.\")\n",
    "        return\n",
    "\n",
    "    print(f\"Starting graph data creation process... Articles: {len(articles)}, Precedents: {len(precedents)}\")\n",
    "    start_time = time.time()\n",
    "\n",
    "    # 배치 처리 설정\n",
    "    batch_size = 500 # 한 번에 처리할 노드/관계 수\n",
    "\n",
    "    # --- 1. 법 조항 노드 처리 ---\n",
    "    articles_batch = []\n",
    "    if articles:\n",
    "        print(\"Processing Article nodes...\")\n",
    "        article_texts = list(articles.values())\n",
    "        article_ids = list(articles.keys())\n",
    "\n",
    "        # 임베딩 (배치 처리)\n",
    "        print(f\"Embedding {len(article_texts)} articles...\")\n",
    "        try:\n",
    "            article_embeddings = embed_model.embed_documents(article_texts, chunk_size=100) # 임베딩 API 배치 크기 조절\n",
    "            print(f\"Successfully embedded {len(article_embeddings)} articles.\")\n",
    "\n",
    "            if len(article_embeddings) != len(article_ids):\n",
    "                 print(f\"Warning: Mismatch between article count ({len(article_ids)}) and embedding count ({len(article_embeddings)}).\")\n",
    "                 # 길이를 맞추거나 오류 처리 필요 - 여기서는 짧은 쪽 기준으로 진행\n",
    "                 min_len = min(len(article_ids), len(article_embeddings))\n",
    "                 article_ids = article_ids[:min_len]\n",
    "                 article_texts = article_texts[:min_len]\n",
    "                 article_embeddings = article_embeddings[:min_len]\n",
    "\n",
    "            for i in range(len(article_ids)):\n",
    "                articles_batch.append({\n",
    "                    \"id\": article_ids[i],\n",
    "                    \"text\": article_texts[i],\n",
    "                    \"embedding\": article_embeddings[i]\n",
    "                })\n",
    "\n",
    "            # Neo4j에 Article 노드 생성/업데이트 (배치)\n",
    "            print(f\"Writing {len(articles_batch)} Article nodes to Neo4j...\")\n",
    "            with driver.session(database=\"neo4j\") as session:\n",
    "                for i in range(0, len(articles_batch), batch_size):\n",
    "                    batch = articles_batch[i:i+batch_size]\n",
    "                    session.run(\n",
    "                        \"\"\"\n",
    "                        UNWIND $batch as article_data\n",
    "                        MERGE (a:Article {id: article_data.id})\n",
    "                        SET a.text = article_data.text,\n",
    "                            a.embedding = article_data.embedding,\n",
    "                            a.last_updated = timestamp()\n",
    "                        \"\"\",\n",
    "                        batch=batch\n",
    "                    )\n",
    "                print(f\"Successfully created/updated {len(articles_batch)} Article nodes.\")\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"Error during Article node processing or embedding: {e}\")\n",
    "            articles_batch = [] # 오류 시 비움\n",
    "\n",
    "    # --- 2. 판례 및 키워드 노드, 관계 처리 ---\n",
    "    precedents_batch = []\n",
    "    relationships_batch = defaultdict(list) # 관계 유형별 리스트\n",
    "    all_keywords = set()\n",
    "    precedents_embedded = 0\n",
    "    precedents_skipped = 0\n",
    "\n",
    "    if precedents:\n",
    "        print(\"Processing Precedent nodes and relationships...\")\n",
    "        # 임베딩할 텍스트 목록 준비\n",
    "        texts_to_embed = []\n",
    "        valid_precedent_indices = []\n",
    "        for i, p in enumerate(precedents):\n",
    "            text = p.get(\"text_for_embedding\")\n",
    "            if text:\n",
    "                texts_to_embed.append(text)\n",
    "                valid_precedent_indices.append(i)\n",
    "            else:\n",
    "                precedents_skipped += 1\n",
    "                # print(f\"Skipping precedent {p.get('case_id', 'N/A')} due to missing text for embedding.\")\n",
    "\n",
    "        if texts_to_embed:\n",
    "            print(f\"Embedding {len(texts_to_embed)} precedents...\")\n",
    "            try:\n",
    "                precedent_embeddings = embed_model.embed_documents(texts_to_embed, chunk_size=100)\n",
    "                print(f\"Successfully embedded {len(precedent_embeddings)} precedents.\")\n",
    "                precedents_embedded = len(precedent_embeddings)\n",
    "\n",
    "                if len(precedent_embeddings) != len(valid_precedent_indices):\n",
    "                    print(f\"Warning: Mismatch between valid precedent count ({len(valid_precedent_indices)}) and embedding count ({len(precedent_embeddings)}). Adjusting...\")\n",
    "                    min_len = min(len(valid_precedent_indices), len(precedent_embeddings))\n",
    "                    valid_precedent_indices = valid_precedent_indices[:min_len]\n",
    "                    precedent_embeddings = precedent_embeddings[:min_len]\n",
    "                    precedents_embedded = min_len # 실제 임베딩된 수 업데이트\n",
    "\n",
    "                embedding_map = {idx: emb for idx, emb in zip(valid_precedent_indices, precedent_embeddings)}\n",
    "\n",
    "                print(\"Preparing Precedent nodes and relationships batch...\")\n",
    "                for i, p in enumerate(tqdm(precedents, desc=\"Preparing batches\")):\n",
    "                    if i not in embedding_map: # 임베딩 실패 또는 스킵된 경우 건너뜀\n",
    "                        continue\n",
    "\n",
    "                    case_id = p.get(\"case_id\")\n",
    "                    if not case_id:\n",
    "                        print(f\"Skipping precedent at index {i} due to missing case_id.\")\n",
    "                        continue\n",
    "\n",
    "                    # 판례 노드 데이터\n",
    "                    precedent_node = {\n",
    "                        \"id\": case_id,\n",
    "                        \"name\": p.get(\"case_name\"),\n",
    "                        \"judgment_summary\": p.get(\"judgment_summary\"),\n",
    "                        \"full_summary\": p.get(\"full_summary\"),\n",
    "                        \"embedding\": embedding_map[i],\n",
    "                        \"keywords_original\": p.get(\"keywords\", []), # 원본 키워드 저장\n",
    "                        \"keywords_extracted\": p.get(\"keywords_extracted\", False) # 자동 추출 여부\n",
    "                    }\n",
    "                    precedents_batch.append(precedent_node)\n",
    "\n",
    "                    # 키워드 및 HAS_KEYWORD 관계\n",
    "                    current_keywords = set(p.get(\"keywords\", []))\n",
    "                    all_keywords.update(current_keywords) # 전체 키워드 집합에 추가\n",
    "                    for kw in current_keywords:\n",
    "                        relationships_batch[\"HAS_KEYWORD\"].append({\n",
    "                            \"case_id\": case_id,\n",
    "                            \"keyword_text\": kw,\n",
    "                            \"source\": \"extracted\" if p.get(\"keywords_extracted\") else \"original\"\n",
    "                        })\n",
    "\n",
    "                    # REFERENCES_ARTICLE 관계 (개선된 매핑 로직)\n",
    "                    referenced_rules = p.get(\"referenced_rules\", [])\n",
    "                    original_refs = p.get(\"referenced_rules_raw\", \"\").split(',') if p.get(\"referenced_rules_raw\") else []\n",
    "                    \n",
    "                    # 매핑 시도 (article ID 집합 필요)\n",
    "                    article_ids_set = set(articles.keys()) if articles else set()\n",
    "                    \n",
    "                    for rule_ref in referenced_rules: # 정제된 rule ID 사용\n",
    "                        match_type = \"unknown\"\n",
    "                        original_full_ref = next((orig.strip() for orig in original_refs if rule_ref in re.sub(r'\\s+', '', orig)), rule_ref) # 원본 참조 찾기\n",
    "\n",
    "                        if rule_ref in article_ids_set:\n",
    "                            match_type = \"exact\"\n",
    "                            relationships_batch[\"REFERENCES_ARTICLE\"].append({\n",
    "                                \"case_id\": case_id, \"article_ref\": rule_ref, \"match_type\": match_type, \"original_ref\": original_full_ref\n",
    "                            })\n",
    "                        else:\n",
    "                            # 부분 또는 매핑된 참조 시도 (예: '제10조'가 '제10조(상해)'에 포함되는 경우)\n",
    "                            partial_match = next((a_id for a_id in article_ids_set if rule_ref in a_id or a_id in rule_ref), None)\n",
    "                            if partial_match:\n",
    "                                match_type = \"partial\" if rule_ref in partial_match or partial_match in rule_ref else \"mapped\" # 더 정교한 매핑 로직 필요 시 추가\n",
    "                                relationships_batch[\"REFERENCES_ARTICLE\"].append({\n",
    "                                    \"case_id\": case_id, \"article_ref\": partial_match, \"match_type\": match_type, \"original_ref\": original_full_ref\n",
    "                                })\n",
    "                            # else: 매칭 실패 시 관계 생성 안 함\n",
    "\n",
    "                    # REFERENCES_CASE 관계\n",
    "                    referenced_cases = p.get(\"referenced_cases\", [])\n",
    "                    for ref_case in referenced_cases:\n",
    "                        if ref_case.get(\"case_id\"): # 유효한 판례 ID가 있는 경우만\n",
    "                            relationships_batch[\"REFERENCES_CASE\"].append({\n",
    "                                \"case_id\": case_id,\n",
    "                                \"referenced_case_id\": ref_case[\"case_id\"],\n",
    "                                \"full_reference\": ref_case.get(\"full_reference\")\n",
    "                            })\n",
    "\n",
    "            except Exception as e:\n",
    "                print(f\"Error during Precedent embedding or batch preparation: {e}\")\n",
    "                # 오류 발생 시 관련 배치 초기화 또는 부분 처리 결정 필요\n",
    "                precedents_batch = []\n",
    "                relationships_batch = defaultdict(list)\n",
    "                all_keywords = set()\n",
    "\n",
    "    # --- 3. Neo4j에 데이터 쓰기 (배치) ---\n",
    "    # 이 부분부터 기존 코드와 연결됩니다.\n",
    "    if precedents_batch: # 이제 precedents_batch가 정의되었습니다.\n",
    "        print(f\"Embedding complete for {precedents_embedded} precedents (skipped {precedents_skipped}). Writing to Neo4j...\")\n",
    "        try:\n",
    "            with driver.session(database=\"neo4j\") as session:\n",
    "                # --- 판례 노드 생성/업데이트 (배치) ---\n",
    "                print(f\"Writing {len(precedents_batch)} Precedent nodes to Neo4j...\")\n",
    "                for i in range(0, len(precedents_batch), batch_size):\n",
    "                    batch = precedents_batch[i:i+batch_size]\n",
    "                    session.run(\n",
    "                        \"\"\"\n",
    "                        UNWIND $batch as p_data\n",
    "                        MERGE (p:Precedent {id: p_data.id})\n",
    "                        SET p.name = p_data.name,\n",
    "                            p.judgment_summary = p_data.judgment_summary,\n",
    "                            p.full_summary = p_data.full_summary,\n",
    "                            p.embedding = p_data.embedding,\n",
    "                            p.keywords_original = p_data.keywords_original,\n",
    "                            p.keywords_extracted = p_data.keywords_extracted,\n",
    "                            p.last_updated = timestamp()\n",
    "                        \"\"\",\n",
    "                        batch=batch\n",
    "                    )\n",
    "                print(f\"Successfully created/updated {len(precedents_batch)} Precedent nodes.\")\n",
    "\n",
    "                # --- 키워드 노드 생성 (존재하지 않는 경우 - 배치) ---\n",
    "                if all_keywords:\n",
    "                    print(f\"Ensuring {len(all_keywords)} Keyword nodes exist...\")\n",
    "                    keyword_list = list(all_keywords)\n",
    "                    for i in range(0, len(keyword_list), batch_size * 10): # 키워드는 더 큰 배치로 처리 가능\n",
    "                        batch = keyword_list[i:i+batch_size*10]\n",
    "                        session.run(\n",
    "                            \"\"\"\n",
    "                            UNWIND $keywords as keyword_text\n",
    "                            MERGE (k:Keyword {text: keyword_text})\n",
    "                            \"\"\",\n",
    "                            keywords=batch\n",
    "                        )\n",
    "                    print(f\"Ensured {len(all_keywords)} Keyword nodes exist.\")\n",
    "\n",
    "                # --- 관계 생성 (배치) ---\n",
    "                rel_types_to_create = [\"HAS_KEYWORD\", \"REFERENCES_ARTICLE\", \"REFERENCES_CASE\"]\n",
    "                total_rels_attempted = 0\n",
    "                total_rels_created = 0\n",
    "\n",
    "                for rel_type in rel_types_to_create:\n",
    "                    rels = relationships_batch.get(rel_type, [])\n",
    "                    if not rels:\n",
    "                        print(f\"No relationships of type {rel_type} to create.\")\n",
    "                        continue\n",
    "\n",
    "                    print(f\"Writing {len(rels)} relationships of type {rel_type}...\")\n",
    "                    created_count_for_type = 0\n",
    "                    rel_batch_size = 5000 # 관계 배치 크기\n",
    "\n",
    "                    # 관계 유형별 Cypher 쿼리 정의\n",
    "                    cypher_query = \"\"\n",
    "                    if rel_type == \"HAS_KEYWORD\":\n",
    "                        cypher_query = \"\"\"\n",
    "                        UNWIND $rels as rel\n",
    "                        MATCH (p:Precedent {id: rel.case_id})\n",
    "                        MATCH (k:Keyword {text: rel.keyword_text})\n",
    "                        MERGE (p)-[r:HAS_KEYWORD {source: rel.source}]->(k)\n",
    "                        SET r.last_updated = timestamp()\n",
    "                        RETURN count(r) as created_count\n",
    "                        \"\"\"\n",
    "                    elif rel_type == \"REFERENCES_ARTICLE\":\n",
    "                        cypher_query = \"\"\"\n",
    "                        UNWIND $rels as rel\n",
    "                        MATCH (p:Precedent {id: rel.case_id})\n",
    "                        MATCH (a:Article {id: rel.article_ref})\n",
    "                        MERGE (p)-[r:REFERENCES_ARTICLE]->(a)\n",
    "                        SET r.match_type = rel.match_type,\n",
    "                            r.original_ref = rel.original_ref,\n",
    "                            r.last_updated = timestamp()\n",
    "                        RETURN count(r) as created_count\n",
    "                        \"\"\"\n",
    "                    elif rel_type == \"REFERENCES_CASE\":\n",
    "                        cypher_query = \"\"\"\n",
    "                        UNWIND $rels as rel\n",
    "                        MATCH (p1:Precedent {id: rel.case_id})\n",
    "                        MATCH (p2:Precedent {id: rel.referenced_case_id})\n",
    "                        MERGE (p1)-[r:REFERENCES_CASE]->(p2)\n",
    "                        SET r.full_reference = rel.full_reference,\n",
    "                            r.last_updated = timestamp()\n",
    "                        RETURN count(r) as created_count\n",
    "                        \"\"\"\n",
    "\n",
    "                    if cypher_query:\n",
    "                        total_rels_attempted += len(rels)\n",
    "                        for i in range(0, len(rels), rel_batch_size):\n",
    "                            batch = rels[i:i+rel_batch_size]\n",
    "                            result = session.run(cypher_query, rels=batch)\n",
    "                            summary = result.consume()\n",
    "                            created_count_for_type += summary.counters.relationships_created\n",
    "                        total_rels_created += created_count_for_type\n",
    "                        print(f\"Attempted to create/update {len(rels)} {rel_type} relationships. Actual created: {created_count_for_type}\")\n",
    "                    else:\n",
    "                        print(f\"Warning: No Cypher query defined for relationship type {rel_type}\")\n",
    "\n",
    "                print(f\"\\nTotal relationships attempted: {total_rels_attempted}\")\n",
    "                print(f\"Total relationships actually created: {total_rels_created}\")\n",
    "\n",
    "                # === 관계 검증 (실제 생성된 관계 수 확인) ===\n",
    "                print(\"\\nVerifying created relationships counts in DB...\")\n",
    "                verification = session.run(\"\"\"\n",
    "                    MATCH ()-[r]->()\n",
    "                    RETURN type(r) as rel_type, count(r) as count\n",
    "                \"\"\")\n",
    "                rel_counts = {record['rel_type']: record['count'] for record in verification}\n",
    "                print(f\"- HAS_KEYWORD: {rel_counts.get('HAS_KEYWORD', 0)} relationships\")\n",
    "                print(f\"- REFERENCES_ARTICLE: {rel_counts.get('REFERENCES_ARTICLE', 0)} relationships\")\n",
    "                print(f\"- REFERENCES_CASE: {rel_counts.get('REFERENCES_CASE', 0)} relationships\")\n",
    "\n",
    "                # 생성 시도 수와 실제 수 비교 (경고용) - 각 타입별로 비교하는 것이 더 정확\n",
    "                # if total_rels_created != sum(rel_counts.values()): # 단순 합계 비교는 부정확할 수 있음\n",
    "                #     print(f\"Warning: Total relationship count mismatch (attempted: {total_rels_created}, actual in DB: {sum(rel_counts.values())})\")\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"Error writing data to Neo4j: {e}\")\n",
    "            import traceback\n",
    "            traceback.print_exc() # 상세 에러 로그 출력\n",
    "\n",
    "    else: # precedents_batch가 비어있는 경우 (precedents 데이터가 없거나 처리 중 오류 발생)\n",
    "        print(\"No valid precedent data processed or available to write to Neo4j.\")\n",
    "\n",
    "    end_time = time.time()\n",
    "    print(f\"Finished graph data creation process in {end_time - start_time:.2f} seconds.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "cb83a4e7-08b3-41ea-a0b5-66db5cee1420",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_query_keywords(text: str, max_keywords: int = 5) -> List[str]:\n",
    "    \"\"\"텍스트에서 간단한 키워드 추출 (개선된 버전: 불용어, 길이, 빈도 고려)\"\"\"\n",
    "    # 간단한 한국어 불용어 목록 (필요시 확장)\n",
    "    stopwords = set([\n",
    "        \"의\", \"가\", \"이\", \"은\", \"들\", \"는\", \"좀\", \"잘\", \"걍\", \"과\", \"도\", \"를\", \"으로\", \"자\", \"에\", \"와\", \"한\", \"하다\",\n",
    "        \"것\", \"그\", \"저\", \"수\", \"때\", \"등\", \"및\", \"제\", \"조\", \"항\", \"관련\", \"대한\", \"대해\", \"위한\", \"있는\", \"하는\",\n",
    "        \"그리고\", \"그러나\", \"그래서\", \"하지만\", \"또는\", \"다른\", \"모든\", \"어떤\", \"누구\", \"무엇\", \"언제\", \"어디서\", \"어떻게\", \"왜\",\n",
    "        \"입니다\", \"습니다\", \"합니다\", \"에서\", \"에게\", \"부터\", \"까지\", \"보다\", \"만\", \"같이\", \"처럼\", \"따라\", \"통해\",\n",
    "        \"경우\", \"문제\", \"질문\", \"답변\", \"선택지\", \"다음\", \"중\", \"가장\", \"적절한\", \"옳은\", \"틀린\"\n",
    "    ])\n",
    "    # 명사형 단어 위주 추출 (정규식 개선)\n",
    "    words = re.findall(r'\\b[가-힣]{2,}\\b', text) # 2글자 이상 한글 단어\n",
    "    keywords = [w for w in words if w not in stopwords and not w.isdigit()]\n",
    "\n",
    "    # 빈도수 계산\n",
    "    counter = Counter(keywords)\n",
    "\n",
    "    # 빈도수 상위 키워드 선택\n",
    "    common_keywords = [k for k, _ in counter.most_common(max_keywords)]\n",
    "    return common_keywords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "838e8739-f396-4236-8672-415f65148637",
   "metadata": {},
   "outputs": [],
   "source": [
    "def retrieve_context_from_graph(driver: Driver, query_text: str, embed_model, top_k: int = 12) -> List[Dict[str, Any]]:\n",
    "    \"\"\"그래프 데이터베이스에서 관련 법 조항 및 판례 검색 (GraphRAG 강화 및 벡터 검색 수정)\"\"\"\n",
    "    if not driver:\n",
    "        print(\"Neo4j driver is not available. Cannot retrieve context.\")\n",
    "        return []\n",
    "    if not embed_model:\n",
    "        print(\"Embedding model is not available. Cannot retrieve context.\")\n",
    "        return []\n",
    "\n",
    "    results = []\n",
    "    all_results_map = {} # 중복 방지 및 결과 통합용 (key: (type, id))\n",
    "\n",
    "    try:\n",
    "        query_embedding = embed_model.embed_query(query_text)\n",
    "        query_keywords = extract_query_keywords(query_text, max_keywords=5)\n",
    "        article_pattern = r'(제\\s*\\d+\\s*조(?:의\\s*\\d+)?(?:\\s*\\(.*?\\))?)'\n",
    "        article_matches = re.findall(article_pattern, query_text)\n",
    "        query_articles = [re.sub(r'\\s+', '', match) for match in article_matches]\n",
    "\n",
    "        print(f\"Query keywords: {query_keywords}\")\n",
    "        print(f\"Query article references: {query_articles}\")\n",
    "\n",
    "        with driver.session(database=\"neo4j\") as session:\n",
    "            # === 다단계 검색 전략 ===\n",
    "\n",
    "            # == 1. 직접 참조된 법조항 검색 ==\n",
    "            if query_articles:\n",
    "                try:\n",
    "                    direct_article_query = \"\"\"\n",
    "                    MATCH (a:Article)\n",
    "                    WHERE a.id IN $article_ids OR\n",
    "                          ANY(id IN $article_ids WHERE a.id STARTS WITH id) OR\n",
    "                          ANY(id IN $article_ids WHERE id STARTS WITH a.id)\n",
    "                    OPTIONAL MATCH (p:Precedent)-[:REFERENCES_ARTICLE]->(a)\n",
    "                    RETURN\n",
    "                        a.id AS id, 'Article' AS type, a.text AS text, 1.0 AS score,\n",
    "                        collect(DISTINCT {id: p.id, name: p.name})[..3] AS related_precedents\n",
    "                    \"\"\"\n",
    "                    direct_article_results = session.run(direct_article_query, article_ids=query_articles)\n",
    "                    count = 0\n",
    "                    for record in direct_article_results:\n",
    "                        key = (\"Article\", record[\"id\"])\n",
    "                        if key not in all_results_map:\n",
    "                            all_results_map[key] = dict(record)\n",
    "                            count += 1\n",
    "                    if count > 0:\n",
    "                        print(f\"Found {count} directly referenced articles (with related precedents).\")\n",
    "                except Exception as e:\n",
    "                    print(f\"Warning: Error searching directly referenced articles: {e}\")\n",
    "\n",
    "            # == 2. 키워드 기반 판례 검색 ==\n",
    "            if query_keywords:\n",
    "                try:\n",
    "                    keyword_query = \"\"\"\n",
    "                    MATCH (k:Keyword)<-[:HAS_KEYWORD]-(p:Precedent)\n",
    "                    WHERE k.text IN $keywords\n",
    "                    WITH p, count(DISTINCT k) AS keyword_match_count WHERE keyword_match_count > 0\n",
    "                    OPTIONAL MATCH (p)-[:REFERENCES_ARTICLE]->(a:Article)\n",
    "                    OPTIONAL MATCH (p)-[:HAS_KEYWORD]->(pk:Keyword)\n",
    "                    WITH p, keyword_match_count,\n",
    "                         collect(DISTINCT a.id)[..3] AS referenced_articles,\n",
    "                         collect(DISTINCT pk.text)[..5] AS keywords\n",
    "                    RETURN\n",
    "                        p.id AS id, 'Precedent' AS type, p.name AS name,\n",
    "                        coalesce(p.full_summary, p.judgment_summary) AS text,\n",
    "                        keyword_match_count / $keyword_count AS score,\n",
    "                        referenced_articles, keywords\n",
    "                    ORDER BY score DESC LIMIT $limit\n",
    "                    \"\"\"\n",
    "                    keyword_results = session.run(\n",
    "                        keyword_query, keywords=query_keywords,\n",
    "                        keyword_count=len(query_keywords), limit=top_k\n",
    "                    )\n",
    "                    count = 0\n",
    "                    for record in keyword_results:\n",
    "                        key = (\"Precedent\", record[\"id\"])\n",
    "                        if key not in all_results_map and record[\"score\"] > 0.1:\n",
    "                            all_results_map[key] = dict(record)\n",
    "                            count += 1\n",
    "                    if count > 0:\n",
    "                         print(f\"Found {count} precedents through keyword matching (with related info).\")\n",
    "                except Exception as e:\n",
    "                    if \"Neo.ClientNotification.Statement.UnknownRelationshipTypeWarning\" in str(e) and \"HAS_KEYWORD\" in str(e):\n",
    "                         print(\"Warning: 'HAS_KEYWORD' relationship type not found during keyword search. Skipping keyword search.\")\n",
    "                    else:\n",
    "                         print(f\"Warning: Error in keyword search: {e}\")\n",
    "\n",
    "            # === 수정: existing_keys 포맷 변경 (리스트의 리스트로) ===\n",
    "            # Cypher에서 리스트 비교를 위해 ['Type', 'id'] 형식 사용\n",
    "            existing_keys_list = [[key[0], key[1]] for key in all_results_map.keys()]\n",
    "\n",
    "            # == 3. 벡터 검색 (Article) + GraphRAG 이웃 정보 ==\n",
    "            try:\n",
    "                article_vector_query = \"\"\"\n",
    "                CALL db.index.vector.queryNodes('article_embedding', $limit, $embedding)\n",
    "                YIELD node AS article, score AS vector_score\n",
    "                // === 수정된 WHERE 절 ===\n",
    "                WHERE NOT ['Article', article.id] IN $existing_keys_list\n",
    "                OPTIONAL MATCH (p:Precedent)-[:REFERENCES_ARTICLE]->(article)\n",
    "                RETURN\n",
    "                    article.id AS id, 'Article' AS type, article.text AS text,\n",
    "                    vector_score AS score,\n",
    "                    collect(DISTINCT {id: p.id, name: p.name})[..3] AS related_precedents\n",
    "                LIMIT $limit\n",
    "                \"\"\"\n",
    "                article_results = session.run(\n",
    "                    article_vector_query,\n",
    "                    embedding=query_embedding,\n",
    "                    existing_keys_list=existing_keys_list, # 수정된 파라미터 이름 및 값\n",
    "                    limit=max(1, top_k // 2) # limit은 1 이상이어야 함\n",
    "                )\n",
    "                count = 0\n",
    "                new_keys_found = [] # 새로 찾은 키 저장\n",
    "                for record in article_results:\n",
    "                    key = (\"Article\", record[\"id\"])\n",
    "                    if key not in all_results_map:\n",
    "                        all_results_map[key] = dict(record)\n",
    "                        new_keys_found.append(['Article', record[\"id\"]])\n",
    "                        count += 1\n",
    "                    elif 'related_precedents' not in all_results_map[key] or not all_results_map[key]['related_precedents']:\n",
    "                         all_results_map[key]['related_precedents'] = record['related_precedents']\n",
    "                if count > 0:\n",
    "                    print(f\"Found {count} new articles through vector search (with related precedents).\")\n",
    "                    # 다음 검색을 위해 existing_keys_list 업데이트\n",
    "                    existing_keys_list.extend(new_keys_found)\n",
    "\n",
    "            except Exception as e_vec1:\n",
    "                # 오류 메시지에 SyntaxError 포함 시 구문 오류로 판단\n",
    "                if \"SyntaxError\" in str(e_vec1):\n",
    "                     print(f\"Critical Error: Vector search syntax error for articles: {e_vec1}\")\n",
    "                     print(\"Skipping Article vector search due to syntax error.\")\n",
    "                else:\n",
    "                     print(f\"Warning: Vector search for articles failed: {e_vec1}\")\n",
    "\n",
    "            # == 4. 벡터 검색 (Precedent) + GraphRAG 이웃 정보 ==\n",
    "            try:\n",
    "                precedent_vector_query = \"\"\"\n",
    "                CALL db.index.vector.queryNodes('precedent_embedding', $limit, $embedding)\n",
    "                YIELD node AS precedent, score AS vector_score\n",
    "                // === 수정된 WHERE 절 ===\n",
    "                WHERE NOT ['Precedent', precedent.id] IN $existing_keys_list\n",
    "                OPTIONAL MATCH (precedent)-[:REFERENCES_ARTICLE]->(a:Article)\n",
    "                OPTIONAL MATCH (precedent)-[:HAS_KEYWORD]->(k:Keyword)\n",
    "                OPTIONAL MATCH (precedent)-[:REFERENCES_CASE]->(ref_p:Precedent)\n",
    "                OPTIONAL MATCH (citing_p:Precedent)-[:REFERENCES_CASE]->(precedent)\n",
    "                WITH precedent, vector_score,\n",
    "                     collect(DISTINCT a.id)[..3] AS referenced_articles,\n",
    "                     collect(DISTINCT k.text)[..5] AS keywords,\n",
    "                     collect(DISTINCT {id: ref_p.id, name: ref_p.name})[..2] AS referenced_precedents,\n",
    "                     collect(DISTINCT {id: citing_p.id, name: citing_p.name})[..2] AS citing_precedents\n",
    "                RETURN\n",
    "                    precedent.id AS id, 'Precedent' AS type, precedent.name AS name,\n",
    "                    coalesce(precedent.full_summary, precedent.judgment_summary) AS text,\n",
    "                    vector_score AS score,\n",
    "                    referenced_articles, keywords, referenced_precedents, citing_precedents\n",
    "                LIMIT $limit\n",
    "                \"\"\"\n",
    "                precedent_results = session.run(\n",
    "                    precedent_vector_query,\n",
    "                    embedding=query_embedding,\n",
    "                    existing_keys_list=existing_keys_list, # 수정된 파라미터 이름 및 값\n",
    "                    limit=max(1, top_k // 2) # limit은 1 이상이어야 함\n",
    "                )\n",
    "                count = 0\n",
    "                for record in precedent_results:\n",
    "                    key = (\"Precedent\", record[\"id\"])\n",
    "                    if key not in all_results_map:\n",
    "                        all_results_map[key] = dict(record)\n",
    "                        count += 1\n",
    "                    else:\n",
    "                        if record['score'] > all_results_map[key].get('score', -1):\n",
    "                             all_results_map[key].update(dict(record))\n",
    "                        else:\n",
    "                            for info_key in ['referenced_articles', 'keywords', 'referenced_precedents', 'citing_precedents']:\n",
    "                                if info_key not in all_results_map[key] or not all_results_map[key][info_key]:\n",
    "                                     all_results_map[key][info_key] = record[info_key]\n",
    "                if count > 0:\n",
    "                    print(f\"Found {count} new precedents through vector search (with related info).\")\n",
    "\n",
    "            except Exception as e_vec2:\n",
    "                 if \"SyntaxError\" in str(e_vec2):\n",
    "                     print(f\"Critical Error: Vector search syntax error for precedents: {e_vec2}\")\n",
    "                     print(\"Skipping Precedent vector search due to syntax error.\")\n",
    "                 else:\n",
    "                     print(f\"Warning: Vector search for precedents failed: {e_vec2}\")\n",
    "\n",
    "            # === 최종 결과 통합 및 정렬 ===\n",
    "            final_results = list(all_results_map.values())\n",
    "            final_results.sort(key=lambda x: x.get(\"score\", -1), reverse=True)\n",
    "            results = final_results[:top_k]\n",
    "\n",
    "            # 텍스트 미리보기 추가\n",
    "            for res in results:\n",
    "                text_preview = res.get(\"text\", \"\")\n",
    "                res[\"text_preview\"] = text_preview[:250] + \"...\" if len(text_preview) > 250 else text_preview\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error during graph retrieval for query '{query_text[:50]}...': {e}\")\n",
    "        results = []\n",
    "\n",
    "    print(f\"Retrieved {len(results)} final context items.\")\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "d1325013-c7e2-4881-a237-91f99226be61",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_text_for_context(text: str, max_len: int = 800) -> str:\n",
    "    \"\"\"주어진 텍스트를 단순 길이 제한으로 처리\"\"\"\n",
    "    if not text:\n",
    "        return \"\"\n",
    "    if len(text) <= max_len:\n",
    "        return text\n",
    "    else:\n",
    "        # 단순 자르기 + 말줄임표\n",
    "        return text[:max_len] + \"...\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "1e64d119-2d7d-45d6-ab02-2aa1092629ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_optimized_context(search_results: List[Dict[str, Any]], question: str) -> str:\n",
    "    \"\"\"검색 결과와 질문을 바탕으로 LLM에 제공할 최적화된 컨텍스트 구성 (GraphRAG 정보 활용)\"\"\"\n",
    "    if not search_results:\n",
    "        return \"관련된 법 조항이나 판례 정보를 찾지 못했습니다. 질문 내용을 바탕으로 직접 답변해주세요.\"\n",
    "\n",
    "    context_parts = [\"### 참고 자료 (질문과 관련성이 높은 순서대로 정렬됨) ###\"]\n",
    "    processed_items = [] # (score, context_string) 저장\n",
    "\n",
    "    for result in search_results:\n",
    "        result_type = result.get(\"type\", \"Unknown\")\n",
    "        result_id = result.get(\"id\", \"N/A\")\n",
    "        score = result.get(\"score\", 0.0)\n",
    "        original_text = result.get(\"text\", \"\")\n",
    "        processed_text = process_text_for_context(original_text, max_len=800 if result_type == 'Article' else 1000)\n",
    "\n",
    "        if not processed_text:\n",
    "            continue\n",
    "\n",
    "        related_info_parts = [] # 관련 정보 문자열 조각들\n",
    "\n",
    "        if result_type == \"Article\":\n",
    "            header = f\"【법조항: {result_id} (관련성 점수: {score:.3f})】\"\n",
    "            # 관련 판례 정보\n",
    "            related_precedents = result.get(\"related_precedents\", [])\n",
    "            if related_precedents:\n",
    "                p_info = [f\"{p.get('name', p.get('id', 'Unknown'))}\" for p in related_precedents]\n",
    "                related_info_parts.append(f\"[관련 판례: {', '.join(p_info)}]\")\n",
    "            # (필요시 다른 관련 정보 추가)\n",
    "\n",
    "        elif result_type == \"Precedent\":\n",
    "            name = result.get(\"name\", \"\")\n",
    "            header = f\"【판례: {name or result_id} (관련성 점수: {score:.3f})】\"\n",
    "            # 참조 법조항\n",
    "            ref_articles = result.get(\"referenced_articles\", [])\n",
    "            if ref_articles:\n",
    "                related_info_parts.append(f\"[참조 법조항: {', '.join(ref_articles)}]\")\n",
    "            # 키워드\n",
    "            keywords = result.get(\"keywords\", [])\n",
    "            if keywords:\n",
    "                related_info_parts.append(f\"[주요 키워드: {', '.join(keywords)}]\")\n",
    "            # 참조 판례\n",
    "            ref_precedents = result.get(\"referenced_precedents\", [])\n",
    "            if ref_precedents:\n",
    "                ref_p_info = [f\"{p.get('name', p.get('id', 'Unknown'))}\" for p in ref_precedents]\n",
    "                related_info_parts.append(f\"[참조 판례: {', '.join(ref_p_info)}]\")\n",
    "            # 인용 판례\n",
    "            citing_precedents = result.get(\"citing_precedents\", [])\n",
    "            if citing_precedents:\n",
    "                cite_p_info = [f\"{p.get('name', p.get('id', 'Unknown'))}\" for p in citing_precedents]\n",
    "                related_info_parts.append(f\"[인용된 판례: {', '.join(cite_p_info)}]\")\n",
    "\n",
    "        else: # Unknown type\n",
    "            header = f\"【정보: {result_id} (관련성 점수: {score:.3f})】\"\n",
    "\n",
    "        # 최종 컨텍스트 조립\n",
    "        full_context = f\"{header}\\n{processed_text}\"\n",
    "        if related_info_parts:\n",
    "            full_context += \"\\n\" + \" \".join(related_info_parts)\n",
    "\n",
    "        processed_items.append((score, full_context))\n",
    "\n",
    "    # 점수 순으로 정렬 (이미 retrieve_context_from_graph에서 정렬됨)\n",
    "    # processed_items.sort(reverse=True) # 필요시 재정렬\n",
    "\n",
    "    # 컨텍스트 길이 제한 적용하여 최종 문자열 생성\n",
    "    max_total_context_len = 3800 # 컨텍스트 길이 약간 늘림\n",
    "    current_total_len = len(context_parts[0])\n",
    "    final_context_count = 0\n",
    "\n",
    "    for _, item_text in processed_items:\n",
    "        if current_total_len + len(item_text) + 2 < max_total_context_len:\n",
    "            context_parts.append(item_text)\n",
    "            current_total_len += len(item_text) + 2\n",
    "            final_context_count += 1\n",
    "        else:\n",
    "            break # 길이 제한 도달\n",
    "\n",
    "    context_parts.append(f\"\\n### 지침: 위의 {final_context_count}개 참고 자료(법조항, 판례 및 관련 정보 포함)를 바탕으로 질문에 가장 적합한 답변을 선택하세요. ###\")\n",
    "\n",
    "    return \"\\n\\n\".join(context_parts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "31d1bb66-e5fb-482b-81d8-412f1f101617",
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_batch_requests(df: pd.DataFrame, retrieved_contexts: Dict[int, List[Dict[str, Any]]], config: Dict) -> List[Dict[str, Any]]:\n",
    "    \"\"\"Batch API 요청 목록 생성 (개선된 프롬프트)\"\"\"\n",
    "    batch_requests = []\n",
    "    if df.empty:\n",
    "        print(\"Evaluation data frame is empty. Cannot prepare batch requests.\")\n",
    "        return []\n",
    "    if not config or not config.get(\"llm_model\"):\n",
    "         print(\"LLM model configuration is missing. Cannot prepare batch requests.\")\n",
    "         return []\n",
    "\n",
    "    print(f\"Preparing batch requests using LLM: {config['llm_model']}...\")\n",
    "\n",
    "    for idx, row in tqdm(df.iterrows(), total=len(df), desc=\"Preparing batch requests\"):\n",
    "        question = row['question']\n",
    "        options = { 'A': row['A'], 'B': row['B'], 'C': row['C'], 'D': row['D'] }\n",
    "        # 인덱스 idx에 해당하는 컨텍스트 가져오기\n",
    "        contexts = retrieved_contexts.get(idx, [])\n",
    "\n",
    "        # 최적화된 컨텍스트 구성\n",
    "        context_str = build_optimized_context(contexts, question)\n",
    "\n",
    "        # --- 개선된 프롬프트 ---\n",
    "        prompt = f\"\"\"**문제:**\n",
    "다음은 한국 형법에 관한 객관식 문제입니다. 제시된 참고 자료를 바탕으로 가장 적절한 답을 선택하고, 그 이유를 간략히 설명해주세요.\n",
    "\n",
    "**질문:** {question}\n",
    "\n",
    "**선택지:**\n",
    "(A) {options['A']}\n",
    "(B) {options['B']}\n",
    "(C) {options['C']}\n",
    "(D) {options['D']}\n",
    "\n",
    "**참고 자료:**\n",
    "{context_str}\n",
    "\n",
    "**답변 형식:**\n",
    "1.  **분석:** 문제의 핵심 쟁점과 관련된 법 조항/판례를 간략히 언급하고, 각 선택지를 검토합니다. (2-3 문장 내외)\n",
    "2.  **최종 답변:** \"정답: [A/B/C/D]\" 형식으로 명확하게 제시합니다.\n",
    "\n",
    "---\n",
    "**예시:**\n",
    "분석: 이 문제는 [핵심 쟁점]에 관한 것으로, 형법 제XX조 및 관련 판례 YYY에 따라 판단해야 합니다. 선택지 (A)는 ... 이유로 타당하고, (B)는 ... 이유로 틀립니다.\n",
    "정답: A\n",
    "---\n",
    "\n",
    "이제 위 형식에 맞춰 답변해주세요.\n",
    "\"\"\"\n",
    "\n",
    "        request = {\n",
    "            # custom_id는 결과 매칭에 사용됨 (Dataframe 인덱스 사용)\n",
    "            \"custom_id\": f\"q_{idx}\",\n",
    "            \"method\": \"POST\",\n",
    "            \"url\": \"/v1/chat/completions\",\n",
    "            \"body\": {\n",
    "                \"model\": config[\"llm_model\"],\n",
    "                \"messages\": [\n",
    "                    {\"role\": \"system\", \"content\": \"당신은 한국 형법 지식을 갖춘 AI 법률 전문가입니다. 주어진 문제, 선택지, 참고 자료를 바탕으로 가장 정확한 답을 논리적으로 분석하고, 지정된 형식에 맞춰 답변을 생성합니다.\"},\n",
    "                    {\"role\": \"user\", \"content\": prompt}\n",
    "                ],\n",
    "                \"max_tokens\": 450, # 분석 내용 포함 위해 약간 늘림\n",
    "                \"temperature\": 0.0, # 최대한 일관되고 결정적인 답변 유도\n",
    "                \"top_p\": 0.1, # Temperature 0과 함께 사용 시 더 결정적\n",
    "            }\n",
    "        }\n",
    "        batch_requests.append(request)\n",
    "\n",
    "    print(f\"Prepared {len(batch_requests)} batch requests.\")\n",
    "    return batch_requests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "81bed279-7b41-451c-85e2-3e846a2df865",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_batch_job(client: OpenAI, batch_requests: List[Dict[str, Any]], config: Dict, timestamp: str) -> Optional[str]:\n",
    "    \"\"\"Batch API 작업을 생성하고 완료될 때까지 모니터링\"\"\"\n",
    "    if not client:\n",
    "        print(\"OpenAI client is not available. Cannot run batch job.\")\n",
    "        return None\n",
    "    if not batch_requests:\n",
    "        print(\"No batch requests to process. Skipping batch job.\")\n",
    "        return None\n",
    "    if not config or not config.get(\"results_dir\"):\n",
    "        print(\"Results directory configuration is missing. Cannot run batch job.\")\n",
    "        return None\n",
    "\n",
    "    # JSONL 파일로 저장\n",
    "    batch_file_path = os.path.join(config[\"results_dir\"], f\"criminal_law_batch_input_{timestamp}.jsonl\")\n",
    "    try:\n",
    "        with open(batch_file_path, 'w', encoding='utf-8') as f:\n",
    "            for request in batch_requests:\n",
    "                f.write(json.dumps(request, ensure_ascii=False) + '\\n')\n",
    "        print(f\"Saved {len(batch_requests)} batch requests to {batch_file_path}\")\n",
    "    except IOError as e:\n",
    "        print(f\"Error saving batch input file: {e}\")\n",
    "        return None\n",
    "\n",
    "    # 배치 파일 업로드\n",
    "    batch_input_file_id = None\n",
    "    try:\n",
    "        print(\"Uploading batch input file to OpenAI...\")\n",
    "        with open(batch_file_path, \"rb\") as f:\n",
    "            batch_input_file = client.files.create(file=f, purpose=\"batch\")\n",
    "        batch_input_file_id = batch_input_file.id\n",
    "        print(f\"Successfully uploaded batch file. File ID: {batch_input_file_id}\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error uploading batch file: {e}\")\n",
    "        return None\n",
    "\n",
    "    # 배치 작업 생성\n",
    "    batch_id = None\n",
    "    try:\n",
    "        print(\"Creating batch job...\")\n",
    "        batch_job = client.batches.create(\n",
    "            input_file_id=batch_input_file_id,\n",
    "            endpoint=\"/v1/chat/completions\",\n",
    "            completion_window=\"24h\", # 24시간 내 완료 요청\n",
    "            metadata={\"description\": f\"KMMLU Criminal Law benchmark {timestamp}\"}\n",
    "        )\n",
    "        batch_id = batch_job.id\n",
    "        print(f\"Successfully created batch job. Job ID: {batch_id}\")\n",
    "        print(f\"Batch job status: {batch_job.status}\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error creating batch job: {e}\")\n",
    "        return None\n",
    "\n",
    "    # 배치 작업 상태 확인 및 대기\n",
    "    print(\"\\nMonitoring batch job progress...\")\n",
    "    start_time = time.time()\n",
    "    status = None\n",
    "    wait_interval = 30 # 초기 대기 시간 (초)\n",
    "    max_wait_time = 3600 * 2 # 최대 대기 시간 (예: 2시간)\n",
    "\n",
    "    while True:\n",
    "        try:\n",
    "            current_time = time.time()\n",
    "            elapsed_time = current_time - start_time\n",
    "            if elapsed_time > max_wait_time:\n",
    "                 print(f\"Maximum wait time ({max_wait_time}s) exceeded. Stopping monitoring.\")\n",
    "                 return batch_id\n",
    "\n",
    "            status = client.batches.retrieve(batch_id)\n",
    "            print(f\"[{datetime.now().strftime('%H:%M:%S')}] Batch job status: {status.status} (Elapsed: {elapsed_time:.0f}s)\")\n",
    "\n",
    "            if status.status == 'completed':\n",
    "                print(\"Batch job completed successfully.\")\n",
    "                break\n",
    "            elif status.status in ['failed', 'cancelled', 'expired']:\n",
    "                print(f\"Batch job ended with status: {status.status}. Check OpenAI dashboard for details.\")\n",
    "                # 실패/취소 시 관련 정보 출력\n",
    "                if hasattr(status, 'errors') and status.errors:\n",
    "                    print(\"Errors reported:\")\n",
    "                    try:\n",
    "                        # 오류가 data 필드 안에 있을 수 있음\n",
    "                        error_data = status.errors.get('data', [])\n",
    "                        for error in error_data[:5]: # 최대 5개 오류 표시\n",
    "                            print(f\"  - Code: {error.get('code')}, Message: {error.get('message')}, Line: {error.get('line')}\")\n",
    "                        if len(error_data) > 5:\n",
    "                            print(f\"  ... and {len(error_data) - 5} more errors.\")\n",
    "                    except Exception as e_parse:\n",
    "                         print(f\"Could not parse error details: {e_parse}\")\n",
    "                return None # 실패/취소 시 None 반환\n",
    "            elif status.status in ['validating', 'in_progress', 'queued', 'cancelling']:\n",
    "                # 진행 중 상태: 대기 시간 점진적 증가\n",
    "                time.sleep(wait_interval)\n",
    "                if wait_interval < 120: # 최대 대기 간격 설정\n",
    "                    wait_interval = min(wait_interval * 1.5, 120)\n",
    "            else:\n",
    "                 print(f\"Unknown batch status: {status.status}. Stopping monitoring.\")\n",
    "                 return None # 알 수 없는 상태\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"Error checking batch status: {e}. Retrying in {wait_interval}s...\")\n",
    "            time.sleep(wait_interval)\n",
    "\n",
    "    end_time = time.time()\n",
    "    total_time = end_time - start_time\n",
    "    print(f\"Batch job monitoring finished in {total_time:.2f} seconds.\")\n",
    "\n",
    "    # 완료 시 파일 정보 출력\n",
    "    if status and status.status == 'completed':\n",
    "        print(f\"Output file ID: {status.output_file_id}\")\n",
    "        print(f\"Error file ID: {status.error_file_id}\")\n",
    "        return batch_id # 성공 시 배치 ID 반환\n",
    "    else:\n",
    "        print(\"Batch job did not complete successfully.\")\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "a78d7209-f692-4945-8164-2b2986c2b737",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_choice_from_response(text: str) -> Optional[str]:\n",
    "    \"\"\"LLM 응답 텍스트에서 최종 선택지(A, B, C, D) 추출 (정확도 향상)\"\"\"\n",
    "    if not text:\n",
    "        return None\n",
    "\n",
    "    text_lower = text.lower() # 소문자 변환\n",
    "\n",
    "    # 1. 가장 명확한 패턴 우선 검색 (\"정답: A\", \"최종 선택: B\" 등)\n",
    "    #    - 대소문자 구분 없이, 콜론/공백 유연하게 처리\n",
    "    #    - 문장 끝 마침표 고려\n",
    "    clear_patterns = [\n",
    "        r'(?:정답|최종\\s*답변|최종\\s*선택)\\s*[:\\s]\\s*([a-d])\\b\\.?',\n",
    "        r'\\b([a-d])\\s*(?:입니다|이다)\\b\\.?\\s*$', # 문장 끝 \"... A입니다.\"\n",
    "        r'^\\s*([a-d])\\b\\.?\\s*$', # 문장 시작 \"A.\" 또는 \"A\"\n",
    "        r'따라서\\s*(?:정답은|답은)?\\s*([a-d])\\b'\n",
    "    ]\n",
    "    for pattern in clear_patterns:\n",
    "        # 전체 텍스트에서 검색\n",
    "        match = re.search(pattern, text, re.IGNORECASE | re.MULTILINE)\n",
    "        if match:\n",
    "            return match.group(1).upper()\n",
    "        # 마지막 줄에서 검색 (중요도 높음)\n",
    "        lines = text.strip().split('\\n')\n",
    "        if lines:\n",
    "             last_line = lines[-1].strip()\n",
    "             match = re.search(pattern, last_line, re.IGNORECASE)\n",
    "             if match:\n",
    "                 return match.group(1).upper()\n",
    "\n",
    "    # 2. 괄호 안의 선택지 패턴 (예: \"(A)\", \"[B]\")\n",
    "    bracket_patterns = [\n",
    "        r'\\( *([a-d]) *\\)',\n",
    "        r'\\[ *([a-d]) *\\]'\n",
    "    ]\n",
    "    # 마지막 문장에서 우선 검색\n",
    "    lines = text.strip().split('\\n')\n",
    "    if lines:\n",
    "        last_line = lines[-1].strip()\n",
    "        for pattern in bracket_patterns:\n",
    "            match = re.search(pattern, last_line, re.IGNORECASE)\n",
    "            if match:\n",
    "                return match.group(1).upper()\n",
    "    # 전체 텍스트에서도 검색\n",
    "    for pattern in bracket_patterns:\n",
    "        matches = re.findall(pattern, text, re.IGNORECASE)\n",
    "        if matches:\n",
    "            # 여러 개 발견 시 마지막 것을 선택 (보통 결론 부분)\n",
    "            return matches[-1].upper()\n",
    "\n",
    "    # 3. 단순 언급 빈도 (최후의 수단, 정확도 낮음)\n",
    "    #    \"A\", \"B\", \"C\", \"D\" 가 텍스트 내에 몇 번 나오는지 카운트\n",
    "    #    긍정/부정 맥락 고려 시도 (간단 버전)\n",
    "    counts = {'A': 0, 'B': 0, 'C': 0, 'D': 0}\n",
    "    positive_keywords = [\"정답\", \"옳은\", \"맞는\", \"타당\", \"적절\", \"선택\"]\n",
    "    negative_keywords = [\"틀린\", \"아닌\", \"오답\", \"부적절\"]\n",
    "\n",
    "    # 문장 단위로 분리하여 분석\n",
    "    sentences = re.split(r'[.!?]\\s+', text)\n",
    "    for sentence in sentences:\n",
    "        sentence_lower = sentence.lower()\n",
    "        for choice in counts.keys():\n",
    "            choice_lower = choice.lower()\n",
    "            # 선택지가 명확히 언급된 경우 (단독 또는 특정 구문과 함께)\n",
    "            if re.search(r'\\b' + choice_lower + r'\\b', sentence_lower):\n",
    "                score = 1\n",
    "                # 긍정 키워드 있으면 가점\n",
    "                if any(pos in sentence_lower for pos in positive_keywords):\n",
    "                    score += 1\n",
    "                # 부정 키워드 있으면 감점\n",
    "                if any(neg in sentence_lower for neg in negative_keywords):\n",
    "                    score -= 1\n",
    "                counts[choice] += score\n",
    "\n",
    "    # 가장 높은 점수를 가진 선택지 반환 (동점 제외)\n",
    "    # 점수가 0 이하인 경우는 제외 (부정적이거나 언급 없는 경우)\n",
    "    positive_counts = {k: v for k, v in counts.items() if v > 0}\n",
    "    if positive_counts:\n",
    "        max_score = max(positive_counts.values())\n",
    "        best_choices = [choice for choice, score in positive_counts.items() if score == max_score]\n",
    "        if len(best_choices) == 1:\n",
    "            return best_choices[0]\n",
    "\n",
    "    # 모든 방법 실패 시 None 반환\n",
    "    print(f\"Could not reliably extract choice from response: {text[:100]}...\")\n",
    "    return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "7edc1d5c-d8ac-4a6a-8a73-52a7e6399869",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_batch_results_and_evaluate(client: OpenAI, batch_id: str, df: pd.DataFrame, config: Dict, timestamp: str) -> Optional[str]:\n",
    "    \"\"\"배치 작업 결과를 다운로드, 처리하고 정확도 평가\"\"\"\n",
    "    if not client:\n",
    "        print(\"OpenAI client is not available. Cannot process results.\")\n",
    "        return None\n",
    "    if not batch_id:\n",
    "        print(\"Batch ID is missing. Cannot process results.\")\n",
    "        return None\n",
    "    if df.empty:\n",
    "        print(\"Evaluation dataframe is empty. Cannot evaluate.\")\n",
    "        return None\n",
    "    if not config or not config.get(\"results_dir\"):\n",
    "        print(\"Results directory configuration is missing. Cannot save results.\")\n",
    "        return None\n",
    "\n",
    "    print(f\"\\nProcessing results for Batch Job ID: {batch_id}\")\n",
    "\n",
    "    try:\n",
    "        # 1. 배치 작업 정보 가져오기\n",
    "        batch_job = client.batches.retrieve(batch_id)\n",
    "        if batch_job.status != 'completed':\n",
    "            print(f\"Batch job {batch_id} did not complete successfully. Status: {batch_job.status}\")\n",
    "            return None\n",
    "\n",
    "        output_file_id = batch_job.output_file_id\n",
    "        error_file_id = batch_job.error_file_id # 오류 파일 ID도 확인\n",
    "        print(f\"Batch job completed. Output file ID: {output_file_id}, Error file ID: {error_file_id}\")\n",
    "\n",
    "        if not output_file_id:\n",
    "             print(\"Error: Batch job completed but no output file ID found.\")\n",
    "             return None\n",
    "\n",
    "        # 2. 결과 파일 다운로드 및 처리\n",
    "        output_file_path = os.path.join(config[\"results_dir\"], f\"criminal_law_batch_output_{timestamp}.jsonl\")\n",
    "        batch_results_raw = []\n",
    "        print(f\"Downloading output file {output_file_id}...\")\n",
    "        try:\n",
    "            file_response = client.files.content(output_file_id)\n",
    "            # 응답 내용을 줄 단위로 처리\n",
    "            raw_content = file_response.text # content 대신 text 사용 (최신 openai 라이브러리)\n",
    "            with open(output_file_path, 'w', encoding='utf-8') as f:\n",
    "                for line in raw_content.strip().split('\\n'):\n",
    "                    if line.strip():\n",
    "                        try:\n",
    "                            batch_results_raw.append(json.loads(line))\n",
    "                            f.write(line + '\\n') # 원본 저장\n",
    "                        except json.JSONDecodeError:\n",
    "                            print(f\"Warning: Could not decode JSON line: {line[:100]}...\")\n",
    "            print(f\"Successfully downloaded and saved {len(batch_results_raw)} raw results to {output_file_path}\")\n",
    "        except Exception as e:\n",
    "            print(f\"Error downloading or saving output file {output_file_id}: {e}\")\n",
    "            return None # 결과 파일 없으면 평가 불가\n",
    "\n",
    "        # 3. 오류 파일 처리 (선택적)\n",
    "        if error_file_id:\n",
    "            error_file_path = os.path.join(config[\"results_dir\"], f\"criminal_law_batch_errors_{timestamp}.jsonl\")\n",
    "            print(f\"Downloading error file {error_file_id}...\")\n",
    "            try:\n",
    "                error_response = client.files.content(error_file_id)\n",
    "                with open(error_file_path, 'w', encoding='utf-8') as f:\n",
    "                    f.write(error_response.text)\n",
    "                print(f\"Downloaded and saved error file to {error_file_path}\")\n",
    "            except Exception as e:\n",
    "                print(f\"Warning: Could not download or save error file {error_file_id}: {e}\")\n",
    "\n",
    "        # 4. 정확도 평가\n",
    "        correct_count = 0\n",
    "        processed_count = 0\n",
    "        results_data = [] # 평가 결과를 저장할 리스트\n",
    "\n",
    "        print(\"Evaluating predictions...\")\n",
    "        for result_entry in tqdm(batch_results_raw, desc=\"Evaluating results\"):\n",
    "            custom_id = result_entry.get('custom_id')\n",
    "            response_body = result_entry.get('response', {}).get('body') if result_entry.get('response') else None\n",
    "            error_body = result_entry.get('error')\n",
    "\n",
    "            if not custom_id:\n",
    "                print(f\"Skipping result entry with missing custom_id: {result_entry}\")\n",
    "                continue\n",
    "\n",
    "            try:\n",
    "                # custom_id에서 원래 DataFrame 인덱스 추출 (q_ 제거)\n",
    "                if custom_id.startswith('q_'):\n",
    "                     idx = int(custom_id[2:])\n",
    "                else:\n",
    "                     print(f\"Warning: Unexpected custom_id format: {custom_id}. Skipping.\")\n",
    "                     continue\n",
    "\n",
    "                # 원본 데이터 가져오기 (존재하는지 확인)\n",
    "                if idx not in df.index:\n",
    "                     print(f\"Warning: Index {idx} from custom_id not found in the original DataFrame. Skipping.\")\n",
    "                     continue\n",
    "\n",
    "                original_row = df.loc[idx]\n",
    "                processed_count += 1\n",
    "                predicted_answer = None\n",
    "                response_text = \"\"\n",
    "                is_correct = False\n",
    "                # 실제 정답 변환 (1->A, 2->B, ...)\n",
    "                actual_answer = chr(64 + original_row['answer'])\n",
    "\n",
    "                if error_body:\n",
    "                    response_text = f\"Error: {error_body.get('message', 'Unknown error')}\"\n",
    "                    predicted_answer = \"Error\" # 예측 실패로 처리\n",
    "                elif response_body and response_body.get('choices'):\n",
    "                    # 첫 번째 choice의 메시지 내용 추출\n",
    "                    message = response_body['choices'][0].get('message', {})\n",
    "                    response_text = message.get('content', '').strip()\n",
    "                    # 응답 텍스트에서 선택지 추출 (개선된 함수 사용)\n",
    "                    predicted_answer = extract_choice_from_response(response_text)\n",
    "\n",
    "                    if predicted_answer:\n",
    "                        is_correct = (predicted_answer == actual_answer)\n",
    "                        if is_correct:\n",
    "                            correct_count += 1\n",
    "                    else:\n",
    "                        predicted_answer = \"Extraction Failed\" # 추출 실패 명시\n",
    "                else:\n",
    "                     response_text = \"Invalid/Empty Response\"\n",
    "                     predicted_answer = \"No Response\"\n",
    "\n",
    "                results_data.append({\n",
    "                    'question_id': idx,\n",
    "                    'question': original_row['question'],\n",
    "                    'A': original_row['A'], 'B': original_row['B'], 'C': original_row['C'], 'D': original_row['D'],\n",
    "                    'predicted': predicted_answer,\n",
    "                    'actual': actual_answer,\n",
    "                    'is_correct': is_correct,\n",
    "                    'response': response_text # LLM의 전체 응답 저장\n",
    "                })\n",
    "            except (ValueError, KeyError, IndexError) as e:\n",
    "                print(f\"Error processing result entry for custom_id {custom_id}: {e}. Entry: {result_entry}\")\n",
    "            except Exception as e:\n",
    "                 print(f\"An unexpected error occurred processing custom_id {custom_id}: {e}\")\n",
    "\n",
    "        if processed_count == 0:\n",
    "            print(\"No results were processed successfully. Evaluation cannot proceed.\")\n",
    "            return None\n",
    "\n",
    "        # 정확도 계산\n",
    "        accuracy = correct_count / processed_count if processed_count > 0 else 0.0\n",
    "        print(f\"\\n--- Evaluation Summary ---\")\n",
    "        print(f\"Total results processed: {processed_count} / {len(df)}\")\n",
    "        print(f\"Correct answers: {correct_count}\")\n",
    "        print(f\"Accuracy: {accuracy:.4f} ({accuracy:.2%})\")\n",
    "\n",
    "        # 5. 상세 결과 저장\n",
    "        results_df = pd.DataFrame(results_data)\n",
    "        results_file_path = os.path.join(config[\"results_dir\"], f\"criminal_law_evaluation_results_{timestamp}.csv\")\n",
    "        try:\n",
    "            results_df.to_csv(results_file_path, index=False, encoding='utf-8-sig') # UTF-8 with BOM for Excel compatibility\n",
    "            print(f\"Saved detailed evaluation results to {results_file_path}\")\n",
    "            return results_file_path # 성공 시 결과 파일 경로 반환\n",
    "        except IOError as e:\n",
    "             print(f\"Error saving evaluation results file: {e}\")\n",
    "             return None\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred during batch result processing and evaluation: {e}\")\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "f989e3e4-c5d5-4b19-8d4b-9fcba8d6fec7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyze_results(results_file_path: str, config: Dict):\n",
    "    \"\"\"평가 결과를 분석 (간소화된 콘솔 출력 버전)\"\"\"\n",
    "    print(f\"\\n--- Analyzing Results from: {results_file_path} ---\")\n",
    "    if not os.path.exists(results_file_path):\n",
    "        print(f\"Error: Results file not found at {results_file_path}\")\n",
    "        return\n",
    "\n",
    "    try:\n",
    "        results_df = pd.read_csv(results_file_path)\n",
    "        # 'is_correct' 컬럼을 boolean 타입으로 변환 (True/False 문자열 처리 포함)\n",
    "        results_df['is_correct'] = results_df['is_correct'].apply(lambda x: str(x).lower() == 'true')\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error reading or processing results file: {e}\")\n",
    "        return\n",
    "\n",
    "    if results_df.empty:\n",
    "        print(\"Results data is empty. Skipping analysis.\")\n",
    "        return\n",
    "\n",
    "    # --- 요약 통계 ---\n",
    "    total_questions = len(results_df)\n",
    "    correct_answers = results_df['is_correct'].sum()\n",
    "    incorrect_answers = total_questions - correct_answers\n",
    "    accuracy = correct_answers / total_questions if total_questions > 0 else 0.0\n",
    "    # 예측 실패/오류 건수\n",
    "    error_predictions = results_df[results_df['predicted'].isin(['Error', 'Extraction Failed', 'No Response'])].shape[0]\n",
    "\n",
    "    print(\"\\n===== Benchmark Result Summary =====\")\n",
    "    print(f\"Model Used: {config.get('llm_model', 'N/A')}\")\n",
    "    print(f\"Total Questions: {total_questions}\")\n",
    "    print(f\"Correct Answers: {correct_answers}\")\n",
    "    print(f\"Incorrect Answers: {incorrect_answers}\")\n",
    "    print(f\"Prediction Errors/Failures: {error_predictions}\")\n",
    "    print(f\"Accuracy (excluding errors): {correct_answers / (total_questions - error_predictions) if (total_questions - error_predictions) > 0 else 0.0:.4f}\")\n",
    "    print(f\"Overall Accuracy (including errors as incorrect): {accuracy:.4f} ({accuracy:.2%})\")\n",
    "\n",
    "    # 정답/오답 분석\n",
    "    options = ['A', 'B', 'C', 'D']\n",
    "    \n",
    "    # 각 선택지별 정답률 계산\n",
    "    prediction_counts = results_df[results_df['predicted'].isin(options)]['predicted'].value_counts()\n",
    "    actual_counts = results_df['actual'].value_counts()\n",
    "    \n",
    "    print(\"\\n--- Choice Distribution ---\")\n",
    "    print(\"Predicted answers distribution:\")\n",
    "    for option in options:\n",
    "        count = prediction_counts.get(option, 0)\n",
    "        print(f\"{option}: {count} ({count/total_questions:.2%})\")\n",
    "    \n",
    "    print(\"\\nActual answers distribution:\")\n",
    "    for option in options:\n",
    "        count = actual_counts.get(option, 0)\n",
    "        print(f\"{option}: {count} ({count/total_questions:.2%})\")\n",
    "    \n",
    "    # 오답 유형 분석\n",
    "    incorrect_df = results_df[(results_df['is_correct'] == False) & (results_df['predicted'].isin(options))]\n",
    "    if not incorrect_df.empty:\n",
    "        incorrect_predictions = incorrect_df['predicted'].value_counts()\n",
    "        print(\"\\n--- Incorrect Answer Analysis ---\")\n",
    "        print(\"Most Frequent Incorrect Predictions:\")\n",
    "        for option, count in incorrect_predictions.items():\n",
    "            print(f\"{option}: {count}\")\n",
    "\n",
    "    # 결과 파일 확인 방법 안내\n",
    "    print(f\"\\nDetailed results have been saved to: {results_file_path}\")\n",
    "    print(\"You can open this CSV file to review all predictions and responses.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2b5fa81-8d45-40ca-83b0-d7ae376a6235",
   "metadata": {},
   "outputs": [],
   "source": [
    "def main():\n",
    "    \"\"\"전체 프로세스 실행 함수\"\"\"\n",
    "    print(\"=== Criminal Law RAG + LLM Evaluation System ===\")\n",
    "    print(f\"Starting at: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\")\n",
    "    start_time = time.time()\n",
    "\n",
    "    # 1. 설정 로드 및 초기화\n",
    "    config, embedding_model, openai_client = load_config_and_initialize()\n",
    "    if not config or not embedding_model or not openai_client:\n",
    "        print(\"Critical initialization failed. Exiting.\")\n",
    "        return\n",
    "\n",
    "    # 2. 데이터 로드\n",
    "    print(\"\\n=== Loading Data ===\")\n",
    "    # articles = {} # 임베딩 이미 했으면 주석해제\n",
    "    # precedents = [] # 임베딩 이미 했으면 주석해제\n",
    "    articles = load_articles_from_pdf(config[\"pdf_path\"]) # 임베딩 이미 했으면 주석\n",
    "    precedents = load_precedents_from_json(config[\"precedent_dir\"], sample_size=3000) # 임베딩 이미 했으면 주석\n",
    "    \n",
    "    # 평가 데이터 로드\n",
    "    try:\n",
    "        eval_df = pd.read_csv(config[\"test_csv_path\"])\n",
    "        # 데이터 검증 (필요한 컬럼 확인 등)\n",
    "        required_cols = ['question', 'A', 'B', 'C', 'D', 'answer']\n",
    "        if not all(col in eval_df.columns for col in required_cols):\n",
    "             raise ValueError(f\"Evaluation CSV must contain columns: {required_cols}\")\n",
    "        # 'answer' 컬럼 타입 확인 및 변환 (숫자 형태 가정)\n",
    "        eval_df['answer'] = pd.to_numeric(eval_df['answer'], errors='coerce')\n",
    "        eval_df.dropna(subset=['answer'], inplace=True) # answer 없는 행 제거\n",
    "        eval_df['answer'] = eval_df['answer'].astype(int)\n",
    "\n",
    "        print(f\"\\nLoaded and validated {len(eval_df)} questions for evaluation from {config['test_csv_path']}\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error loading evaluation data: {e}\")\n",
    "        return\n",
    "\n",
    "    # 3. Neo4j 연결 및 설정\n",
    "    print(\"\\n=== Connecting to Neo4j ===\")\n",
    "    neo4j_driver = connect_neo4j(config[\"neo4j_uri\"], (config[\"neo4j_username\"], config[\"neo4j_password\"]))\n",
    "    if not neo4j_driver:\n",
    "        print(\"Failed to connect to Neo4j. Exiting.\")\n",
    "        return\n",
    "    \n",
    "    setup_neo4j_constraints_and_indexes(neo4j_driver, config[\"embedding_dimension\"])\n",
    "\n",
    "    # 4. 그래프 데이터 생성\n",
    "    print(\"\\n=== Creating Graph Data ===\")\n",
    "    create_graph_nodes_and_relationships(neo4j_driver, articles, precedents, embedding_model) # 임베딩 이미 했으면 주석\n",
    "\n",
    "    # 5. RAG 컨텍스트 검색\n",
    "    print(\"\\n=== Performing RAG Context Search ===\")\n",
    "    retrieved_contexts = {}\n",
    "    for idx, row in tqdm(eval_df.iterrows(), total=len(eval_df), desc=\"Retrieving contexts\"):\n",
    "        question = row['question']\n",
    "        try:\n",
    "            contexts = retrieve_context_from_graph(neo4j_driver, question, embedding_model, top_k=8)\n",
    "            retrieved_contexts[idx] = contexts\n",
    "        except Exception as e:\n",
    "            print(f\"Error during RAG search for question index {idx}: {e}\")\n",
    "            retrieved_contexts[idx] = [] # 오류 발생 시 빈 컨텍스트\n",
    "\n",
    "    print(f\"Completed RAG search. Retrieved contexts for {len(retrieved_contexts)} questions.\")\n",
    "\n",
    "    # 6. 배치 요청 준비\n",
    "    print(\"\\n=== Preparing Batch Requests ===\")\n",
    "    batch_requests = prepare_batch_requests(eval_df, retrieved_contexts, config)\n",
    "\n",
    "    # 7. Batch API 실행\n",
    "    print(\"\\n=== Executing Batch API Job ===\")\n",
    "    timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "    batch_start = time.time()\n",
    "    batch_id = run_batch_job(openai_client, batch_requests, config, timestamp)\n",
    "    batch_end = time.time()\n",
    "    \n",
    "    if not batch_id:\n",
    "        print(\"Batch job execution failed. Exiting.\")\n",
    "        close_neo4j(neo4j_driver)\n",
    "        return\n",
    "\n",
    "    # 8. 결과 처리 및 평가\n",
    "    print(\"\\n=== Processing Results and Evaluating ===\")\n",
    "    results_file = process_batch_results_and_evaluate(openai_client, batch_id, eval_df, config, timestamp)\n",
    "    \n",
    "    if not results_file:\n",
    "        print(\"Result processing failed. Exiting.\")\n",
    "        close_neo4j(neo4j_driver)\n",
    "        return\n",
    "\n",
    "    # 9. 결과 분석\n",
    "    print(\"\\n=== Analyzing Results ===\")\n",
    "    analyze_results(results_file, config)\n",
    "\n",
    "    # 10. 정리 및 마무리\n",
    "    close_neo4j(neo4j_driver)\n",
    "\n",
    "    end_time = time.time()\n",
    "    batch_duration = batch_end - batch_start\n",
    "    code_duration = (end_time - start_time) - batch_duration\n",
    "    \n",
    "    print(\"\\n=== Process Complete ===\")\n",
    "    print(f\"Total execution time: {end_time - start_time:.2f} seconds\")\n",
    "    print(f\"Batch API execution time: {batch_duration:.2f} seconds\")\n",
    "    print(f\"Code execution time (excluding Batch API): {code_duration:.2f} seconds\")\n",
    "    print(f\"Results saved to: {results_file}\")\n",
    "    print(f\"Finished at: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
