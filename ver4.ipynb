{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "eaa53ffc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install necessary libraries\n",
    "# %pip install langchain langchain_community langchain_openai neo4j python-dotenv pypdf tiktoken\n",
    "\n",
    "import os\n",
    "import json\n",
    "import re\n",
    "from dotenv import load_dotenv\n",
    "from langchain_community.document_loaders import PyPDFLoader\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain_core.documents import Document\n",
    "from langchain_openai import OpenAIEmbeddings\n",
    "from neo4j import GraphDatabase\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0cae8ab6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load environment variables (ensure NEO4J_URI, NEO4J_USERNAME, NEO4J_PASSWORD, OPENAI_API_KEY are set in your .env file)\n",
    "load_dotenv()\n",
    "\n",
    "NEO4J_URI = os.getenv(\"NEO4J_URI\")\n",
    "NEO4J_USERNAME = os.getenv(\"NEO4J_USERNAME\")\n",
    "NEO4J_PASSWORD = os.getenv(\"NEO4J_PASSWORD\")\n",
    "\n",
    "# NEO4J_URI = os.getenv(\"NEO4J_URI\", \"bolt://localhost:7687\")\n",
    "# NEO4J_USERNAME = os.getenv(\"NEO4J_USERNAME\", \"neo4j\")\n",
    "# NEO4J_PASSWORD = os.getenv(\"NEO4J_PASSWORD\", \"password\")\n",
    "\n",
    "OPENAI_API_KEY = os.getenv(\"OPENAI_API_KEY\")\n",
    "\n",
    "# Embedding model setup\n",
    "embedding_model = OpenAIEmbeddings(model=\"text-embedding-3-small\", api_key=OPENAI_API_KEY)\n",
    "embedding_dimension = 1536 # Dimension for text-embedding-3-small\n",
    "\n",
    "# Data paths\n",
    "pdf_path = './dataset/criminal-law.pdf'\n",
    "precedent_dir = './dataset/precedent_label/' # Directory containing JSON files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "65cd2b64",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed 71 articles from PDF.\n",
      "\n",
      "--- Example Article: 제4조(국외에 있는 내국선박 등에서 외국인이 범한 죄) ---\n",
      "본법은 대한민국영역 외에 있는 대한민\n",
      "국의 선박 또는 항공기 내에서 죄를 범한 외국인에게 적용한다.\n",
      " \n",
      "제5조(외국인의 국외범)제5조(외국인의 국외범) 본법은 대한민국영역 외에서 다음에 기재한 죄를 범한 외국인에게 적용\n",
      "한다.\n",
      "1. 내란의 죄\n",
      "2. 외환의 죄\n",
      "3. 국기에 관한 죄\n",
      "4. 통화에 관한 죄\n",
      "5. 유가증권, 우표와 인지에 관한 죄\n",
      "6. 문서에 관한 죄 중 제225조 내지 제230조\n",
      "7. 인장에 관한 죄 중 제238조\n",
      " \n",
      "제6조(대한민국과 대한민국 국민에 대한 국외범)제6조(대한민국과 대한민국 국민에 대한 국외범) 본법은 대한민국영역 외에서 대한민국 또는 대\n",
      "한민국 국민에 대하여 전조에 기재한 이외의 죄를 범한 외국인에게 적용한다. 단 행위지의 법\n",
      "률에 의하여 범죄를 구성하지 아니하거나 소추 또는 형의 집행을 면제할 경우에는 예외로 한\n",
      "다.\n",
      " \n",
      "형법\n",
      "[시행 2010.10.16] [법률 제10259호, 2010.4.15, 일부개정]\n",
      "법제처                      ...\n"
     ]
    }
   ],
   "source": [
    "# Load PDF\n",
    "loader = PyPDFLoader(pdf_path)\n",
    "# pages = loader.load()[2:] # Skip first two pages as before\n",
    "pages = loader.load()\n",
    "full_text = \"\\n\".join(page.page_content for page in pages)\n",
    "\n",
    "# Text Splitter (refined for graph structure)\n",
    "# We'll primarily focus on splitting by Article for node creation\n",
    "article_pattern_precise = re.compile(r'(제\\d+조(?:의\\d+)?\\s*\\(.+?\\))\\s*') # More precise pattern to capture article header\n",
    "separators = [\n",
    "    r\"(제\\d+조(?:의\\d+)?\\s*\\(.+?\\))\", # 조 (Article) - Primary separator\n",
    "    r\"(제\\d+장 [^\\\\n]+)\",             # 장 (Chapter)\n",
    "    r\"(제\\d+편 [^\\\\n]+)\",             # 편 (Edition)\n",
    "    \"\\n\\n\",                         # Paragraph\n",
    "    \"\\n\",                           # Line break\n",
    "    \" \",                            # Space\n",
    "]\n",
    "text_splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=1000, # Adjust as needed\n",
    "    chunk_overlap=150, # Adjust as needed\n",
    "    length_function=len,\n",
    "    separators=separators,\n",
    "    is_separator_regex=True,\n",
    ")\n",
    "\n",
    "raw_chunks = text_splitter.split_text(full_text)\n",
    "\n",
    "# Process chunks to associate content with articles\n",
    "articles = {}\n",
    "current_article_id = None\n",
    "current_content = \"\"\n",
    "\n",
    "for chunk in raw_chunks:\n",
    "    match = article_pattern_precise.match(chunk)\n",
    "    if match:\n",
    "        # If we encounter a new article, save the previous one (if any)\n",
    "        if current_article_id:\n",
    "            articles[current_article_id] = current_content.strip()\n",
    "\n",
    "        # Start the new article\n",
    "        current_article_id = match.group(1).strip()\n",
    "        # Remove the matched article header from the beginning of the chunk content\n",
    "        current_content = article_pattern_precise.sub(\"\", chunk, count=1)\n",
    "    else:\n",
    "        # Append chunk content to the current article\n",
    "        if current_article_id: # Only append if we are already inside an article\n",
    "             current_content += \"\\n\" + chunk\n",
    "\n",
    "# Save the last processed article\n",
    "if current_article_id:\n",
    "    articles[current_article_id] = current_content.strip()\n",
    "\n",
    "\n",
    "print(f\"Processed {len(articles)} articles from PDF.\")\n",
    "# Example: Print the first processed article\n",
    "if articles:\n",
    "    first_article_id = list(articles.keys())[0]\n",
    "    print(f\"\\n--- Example Article: {first_article_id} ---\")\n",
    "    print(articles[first_article_id][:500] + \"...\")\n",
    "\n",
    "# Convert to Document objects for potential later use or consistency\n",
    "article_docs = [Document(page_content=content, metadata={\"article_id\": article_id}) for article_id, content in articles.items()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "edc2353d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 5404 precedents.\n",
      "\n",
      "--- Example Precedent ---\n",
      "{\n",
      "  \"case_id\": \"88도2209\",\n",
      "  \"case_name\": \"매장및묘지등에관한법률위반, 사문서위조, 동행사, 조세범처벌법위반, 특정범죄가중처벌등에관한법률위반\",\n",
      "  \"judgment_summary\": \"가. 작성명의자의 인영이나 주민등록번호의 등재가 누락된 문서가 사문서위조죄의 객체인 사문서에 해당하는지 여부\\n나. 사문서위조 및 동행사죄가 조세범처벌법 제9조 소정의 조세포탈의 수단으로 행해진 경우 후자의 죄에 흡수되는지 여부(소극)\",\n",
      "  \"full_summary\": \"사문서의 작성명의자의 인장이 압날되지 아니하고 주민등록번호가 기재되지 않았다고 하더라도, 일반인으로 하여금 그 작상명의자가 진정하게 작성한 사문서로 믿기에 충분할 정도의 형식과 외관을 갖추었으면 사문서위조죄 및 동행사죄의 객체가 되는 사문서라고 보아야 할 것이고, 사문서위조 및 동행사죄가 조세범처벌법 제9조 제1항 소정의 “사기 기타 부정한 행위로써 조세를 포탈”하기 위한 수단으로 행하여졌다고 하여 조세범처벌법 제9조 소정의 조세포탈죄에 흡수된다고 볼 수도 없는 것이므로, 논지는 이유가 없다.\",\n",
      "  \"keywords\": [\n",
      "    \"사문서위조\",\n",
      "    \"동행사\"\n",
      "  ],\n",
      "  \"referenced_rules\": [\n",
      "    \"제231조\",\n",
      "    \"제9조\",\n",
      "    \"제37조\",\n",
      "    \"제234조\"\n",
      "  ],\n",
      "  \"referenced_cases\": []\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "# Load precedent JSON files\n",
    "precedents = []\n",
    "for filename in os.listdir(precedent_dir):\n",
    "    if filename.endswith(\".json\"):\n",
    "        filepath = os.path.join(precedent_dir, filename)\n",
    "        try:\n",
    "            with open(filepath, 'r', encoding='utf-8') as f:\n",
    "                data = json.load(f)\n",
    "                # Extract relevant fields\n",
    "                precedent_info = {\n",
    "                    \"case_id\": data.get(\"info\", {}).get(\"caseNoID\", filename.replace(\".json\", \"\")),\n",
    "                    \"case_name\": data.get(\"info\", {}).get(\"caseNm\"),\n",
    "                    \"judgment_summary\": data.get(\"jdgmn\"),\n",
    "                    \"full_summary\": \" \".join([s.get(\"summ_contxt\", \"\") for s in data.get(\"Summary\", [])]),\n",
    "                    \"keywords\": [kw.get(\"keyword\") for kw in data.get(\"keyword_tagg\", []) if kw.get(\"keyword\")],\n",
    "                    \"referenced_rules\": data.get(\"Reference_info\", {}).get(\"reference_rules\", \"\").split(',') if data.get(\"Reference_info\", {}).get(\"reference_rules\") else [],\n",
    "                    \"referenced_cases\": data.get(\"Reference_info\", {}).get(\"reference_court_case\", \"\").split(',') if data.get(\"Reference_info\", {}).get(\"reference_court_case\") else [],\n",
    "                }\n",
    "                # Clean up referenced rules (extract article numbers)\n",
    "                cleaned_rules = []\n",
    "                rule_pattern = re.compile(r'제\\d+조(?:의\\d+)?') # Pattern to find \"제X조\" or \"제X조의Y\"\n",
    "                for rule in precedent_info[\"referenced_rules\"]:\n",
    "                    # Find all matches in the rule string\n",
    "                    matches = rule_pattern.findall(rule.strip())\n",
    "                    cleaned_rules.extend(matches)\n",
    "                precedent_info[\"referenced_rules\"] = list(set(cleaned_rules)) # Keep unique article numbers\n",
    "\n",
    "                precedents.append(precedent_info)\n",
    "        except json.JSONDecodeError:\n",
    "            print(f\"Warning: Could not decode JSON from {filename}\")\n",
    "        except Exception as e:\n",
    "            print(f\"Error processing {filename}: {e}\")\n",
    "\n",
    "\n",
    "print(f\"Loaded {len(precedents)} precedents.\")\n",
    "# Example: Print the first loaded precedent\n",
    "if precedents:\n",
    "    print(\"\\n--- Example Precedent ---\")\n",
    "    print(json.dumps(precedents[0], indent=2, ensure_ascii=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d3b4b74e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Randomly selected 1000 precedents out of 5404 total precedents.\n"
     ]
    }
   ],
   "source": [
    "# 로드된 판례 중 무작위로 1,000개만 선택\n",
    "import random\n",
    "random.seed(42)  # 재현성을 위한 시드 설정 (선택사항)\n",
    "\n",
    "# 전체 판례 수 저장\n",
    "total_precedents = len(precedents)\n",
    "\n",
    "# 무작위로 1,000개 선택 (또는 전체 판례 수가 1,000개보다 적다면 모두 선택)\n",
    "sample_size = min(1000, total_precedents)\n",
    "precedents = random.sample(precedents, sample_size)\n",
    "\n",
    "print(f\"Randomly selected {len(precedents)} precedents out of {total_precedents} total precedents.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "af35f38d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully connected to Neo4j.\n",
      "Article vector index created or already exists.\n",
      "Precedent vector index created or already exists.\n",
      "Waiting for indexes to populate...\n",
      "Indexes should be online.\n"
     ]
    }
   ],
   "source": [
    "# Connect to Neo4j\n",
    "try:\n",
    "    driver = GraphDatabase.driver(NEO4J_URI, auth=(NEO4J_USERNAME, NEO4J_PASSWORD))\n",
    "    driver.verify_connectivity()\n",
    "    print(\"Successfully connected to Neo4j.\")\n",
    "except Exception as e:\n",
    "    print(f\"Failed to connect to Neo4j: {e}\")\n",
    "    # Stop execution if connection fails\n",
    "    raise\n",
    "\n",
    "# Create constraints and indexes for faster lookups and embedding search\n",
    "def setup_neo4j(driver, dimension):\n",
    "    with driver.session(database=\"neo4j\") as session:\n",
    "        # Constraints for uniqueness\n",
    "        session.run(\"CREATE CONSTRAINT article_id IF NOT EXISTS FOR (a:Article) REQUIRE a.id IS UNIQUE\")\n",
    "        session.run(\"CREATE CONSTRAINT precedent_id IF NOT EXISTS FOR (p:Precedent) REQUIRE p.id IS UNIQUE\")\n",
    "        session.run(\"CREATE CONSTRAINT keyword_text IF NOT EXISTS FOR (k:Keyword) REQUIRE k.text IS UNIQUE\")\n",
    "\n",
    "        # Vector index for Articles\n",
    "        try:\n",
    "            session.run(\n",
    "                \"CREATE VECTOR INDEX article_embedding IF NOT EXISTS \"\n",
    "                \"FOR (a:Article) ON (a.embedding) \"\n",
    "                f\"OPTIONS {{indexConfig: {{`vector.dimensions`: {dimension}, `vector.similarity_function`: 'cosine'}}}}\"\n",
    "            )\n",
    "            print(\"Article vector index created or already exists.\")\n",
    "        except Exception as e:\n",
    "            print(f\"Error creating Article vector index (may require Neo4j 5.11+ and APOC): {e}\")\n",
    "            print(\"Continuing without vector index creation for Article.\")\n",
    "\n",
    "\n",
    "        # Vector index for Precedents\n",
    "        try:\n",
    "            session.run(\n",
    "                \"CREATE VECTOR INDEX precedent_embedding IF NOT EXISTS \"\n",
    "                \"FOR (p:Precedent) ON (p.embedding) \"\n",
    "                f\"OPTIONS {{indexConfig: {{`vector.dimensions`: {dimension}, `vector.similarity_function`: 'cosine'}}}}\"\n",
    "            )\n",
    "            print(\"Precedent vector index created or already exists.\")\n",
    "        except Exception as e:\n",
    "            print(f\"Error creating Precedent vector index (may require Neo4j 5.11+ and APOC): {e}\")\n",
    "            print(\"Continuing without vector index creation for Precedent.\")\n",
    "\n",
    "        # Wait for indexes to come online (important!)\n",
    "        print(\"Waiting for indexes to populate...\")\n",
    "        session.run(\"CALL db.awaitIndexes(300)\") # Wait up to 300 seconds\n",
    "        print(\"Indexes should be online.\")\n",
    "\n",
    "\n",
    "setup_neo4j(driver, embedding_dimension)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "091dcd1e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating 71 Article nodes...\n",
      "  Processed 50/71 articles...\n",
      "Finished creating 71 Article nodes in 42.63 seconds.\n"
     ]
    }
   ],
   "source": [
    "# Create Article nodes and generate/store embeddings\n",
    "def create_article_nodes(driver, articles_dict, embed_model):\n",
    "    print(f\"Creating {len(articles_dict)} Article nodes...\")\n",
    "    count = 0\n",
    "    start_time = time.time()\n",
    "    with driver.session(database=\"neo4j\") as session:\n",
    "        for article_id, content in articles_dict.items():\n",
    "            if not content: # Skip empty content\n",
    "                print(f\"Skipping article {article_id} due to empty content.\")\n",
    "                continue\n",
    "            try:\n",
    "                # Generate embedding\n",
    "                embedding = embed_model.embed_query(content)\n",
    "\n",
    "                # Create node in Neo4j\n",
    "                session.run(\n",
    "                    \"\"\"\n",
    "                    MERGE (a:Article {id: $article_id})\n",
    "                    SET a.text = $content,\n",
    "                        a.embedding = $embedding\n",
    "                    \"\"\",\n",
    "                    article_id=article_id,\n",
    "                    content=content,\n",
    "                    embedding=embedding\n",
    "                )\n",
    "                count += 1\n",
    "                if count % 50 == 0:\n",
    "                    print(f\"  Processed {count}/{len(articles_dict)} articles...\")\n",
    "            except Exception as e:\n",
    "                print(f\"Error processing article {article_id}: {e}\")\n",
    "\n",
    "    end_time = time.time()\n",
    "    print(f\"Finished creating {count} Article nodes in {end_time - start_time:.2f} seconds.\")\n",
    "\n",
    "create_article_nodes(driver, articles, embedding_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "881f8fc4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating 1000 Precedent nodes and relationships...\n",
      "  Processed 100/1000 precedents...\n",
      "  Processed 200/1000 precedents...\n",
      "  Processed 300/1000 precedents...\n",
      "  Processed 400/1000 precedents...\n",
      "  Processed 500/1000 precedents...\n",
      "  Processed 600/1000 precedents...\n",
      "  Processed 700/1000 precedents...\n",
      "  Processed 800/1000 precedents...\n",
      "  Processed 900/1000 precedents...\n",
      "  Processed 1000/1000 precedents...\n",
      "Finished creating 1000 Precedent nodes and relationships in 1177.54 seconds.\n"
     ]
    }
   ],
   "source": [
    "# Create Precedent nodes, Keyword nodes, and relationships\n",
    "def create_precedent_nodes_and_relationships(driver, precedents_list, embed_model):\n",
    "    print(f\"Creating {len(precedents_list)} Precedent nodes and relationships...\")\n",
    "    count = 0\n",
    "    start_time = time.time()\n",
    "    with driver.session(database=\"neo4j\") as session:\n",
    "        for precedent in precedents_list:\n",
    "            # Use full_summary for embedding, or judgment_summary if full is empty\n",
    "            text_to_embed = precedent.get(\"full_summary\") or precedent.get(\"judgment_summary\")\n",
    "            if not text_to_embed:\n",
    "                print(f\"Skipping precedent {precedent.get('case_id')} due to empty summary.\")\n",
    "                continue\n",
    "\n",
    "            try:\n",
    "                # Generate embedding\n",
    "                embedding = embed_model.embed_query(text_to_embed)\n",
    "\n",
    "                # Create Precedent node\n",
    "                session.run(\n",
    "                    \"\"\"\n",
    "                    MERGE (p:Precedent {id: $case_id})\n",
    "                    SET p.name = $case_name,\n",
    "                        p.judgment_summary = $judgment_summary,\n",
    "                        p.full_summary = $full_summary,\n",
    "                        p.embedding = $embedding\n",
    "                    \"\"\",\n",
    "                    case_id=precedent[\"case_id\"],\n",
    "                    case_name=precedent[\"case_name\"],\n",
    "                    judgment_summary=precedent[\"judgment_summary\"],\n",
    "                    full_summary=precedent[\"full_summary\"],\n",
    "                    embedding=embedding\n",
    "                )\n",
    "\n",
    "                # Create Keyword nodes and relationships\n",
    "                for keyword_text in precedent[\"keywords\"]:\n",
    "                    session.run(\n",
    "                        \"\"\"\n",
    "                        MERGE (k:Keyword {text: $keyword_text})\n",
    "                        WITH k\n",
    "                        MATCH (p:Precedent {id: $case_id})\n",
    "                        MERGE (p)-[:HAS_KEYWORD]->(k)\n",
    "                        \"\"\",\n",
    "                        keyword_text=keyword_text,\n",
    "                        case_id=precedent[\"case_id\"]\n",
    "                    )\n",
    "\n",
    "                # Create relationships to referenced Articles\n",
    "                # Note: This uses the cleaned article IDs extracted earlier\n",
    "                # It tries to match based on the \"제X조\" format.\n",
    "                for article_id_ref in precedent[\"referenced_rules\"]:\n",
    "                     # Find Article nodes that START WITH the referenced ID (e.g., \"제21조\" should match \"제21조(정당방위)\")\n",
    "                     # This is less precise but necessary if the exact title isn't in the reference.\n",
    "                    session.run(\n",
    "                        \"\"\"\n",
    "                        MATCH (p:Precedent {id: $case_id})\n",
    "                        MATCH (a:Article)\n",
    "                        WHERE a.id STARTS WITH $article_id_ref\n",
    "                        MERGE (p)-[:REFERENCES_ARTICLE]->(a)\n",
    "                        \"\"\",\n",
    "                        case_id=precedent[\"case_id\"],\n",
    "                        article_id_ref=article_id_ref # Use the extracted \"제X조\"\n",
    "                    )\n",
    "\n",
    "                # Potential: Create relationships to other referenced Precedents (if needed)\n",
    "                # for ref_case_id in precedent[\"referenced_cases\"]:\n",
    "                #    session.run(...) # MERGE (p)-[:REFERENCES_CASE]->(other_p:Precedent {id: ref_case_id})\n",
    "\n",
    "                count += 1\n",
    "                if count % 100 == 0:\n",
    "                    print(f\"  Processed {count}/{len(precedents_list)} precedents...\")\n",
    "\n",
    "            except Exception as e:\n",
    "                print(f\"Error processing precedent {precedent.get('case_id')}: {e}\")\n",
    "\n",
    "    end_time = time.time()\n",
    "    print(f\"Finished creating {count} Precedent nodes and relationships in {end_time - start_time:.2f} seconds.\")\n",
    "\n",
    "\n",
    "create_precedent_nodes_and_relationships(driver, precedents, embedding_model)\n",
    "\n",
    "# Close the driver connection when done\n",
    "# driver.close() # Keep it open for querying in the next step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d358b800",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Querying RAG for: '정당방위의 요건은 무엇인가?' ---\n",
      "Query completed in 1.46 seconds.\n",
      "\n",
      "--- Top Results ---\n",
      "1. Type: Precedent, ID: 2009도2114, Score: 0.6935\n",
      "   Name: 특수공무집행방해[변경된죄명:폭력행위등처벌에관한법률위반(공동폭행)]\n",
      "   Keywords: ['사회상규에 위배', '소극적인 방어행위']\n",
      "   Referenced Articles: ['제260조(폭행, 존속폭행)']\n",
      "   Text Preview: 비록 경찰관들의 위법한 상경 제지 행위에 대항하기 위하여 한 것이라 하더라도, 피고인들이 다른 시위참가자들과 공동하여 위와 같이 경찰관들을 때리고 진압방패와 채증장비를 빼앗는 등의 폭행행위를 한 것은 소극적인 방어행위를 넘어서 공격의 의사를 포함하여 이루어진 것으로서 그 수단과 방법에 있어서 상당성이 인정된다고 보기 어려우며 긴급하고 불가피한 수단이었다고 볼 수도 없으므로, 이를 사회상규에 위배되지 아니하는 정당행위나 현재의 부당한 침해를 방어하기 위한 정당방위에 해당한다고 볼 수 없다.\n",
      "그럼에도 불구하고, 원심은 그 판시와 같은 ...\n",
      "--------------------\n",
      "2. Type: Precedent, ID: 83누383, Score: 0.6789\n",
      "   Name: 영업정지처분무효확인\n",
      "   Keywords: ['법령 위반행위']\n",
      "   Referenced Articles: ['제1조 (시행일)']\n",
      "   Text Preview: 정당한 절차에 의하지 않고 구두에 의한 하도급계약을 체결하여 공사를 시작한 때에건설업법 제34조 제3항의 위반행위를 범한 것이 되니 그 위반행위를 이유로 한 행정상의 제재처분(행위당시에는 필요적 취소사유)을 하려면 그 위반행위 이후 법령의 변경에 의하여 처분의 종류를 달리(영업정지 사유로) 규정하였다 하더라도 그 법률적용에 관한특별한 규정이 없다면 위반행위 당시에 시행되던 법령을 근거로 처분을 하여야 마땅하다....\n",
      "--------------------\n",
      "3. Type: Article, ID: 제49조(몰수의 부가성), Score: 0.6273\n",
      "   Text Preview: 제49조(몰수의 부가성) 몰수는 타형에 부가하여 과한다. 단, 행위자에게 유죄의 재판을 아니할\n",
      "때에도 몰수의 요건이 있는 때에는 몰수만을 선고할 수 있다.\n",
      " \n",
      "제50조(형의 경중)제50조(형의 경중) ① 형의 경중은 제41조 기재의 순서에 의한다. 단, 무기금고와 유기징역은\n",
      "금고를 중한 것으로 하고 유기금고의 장기가 유기징역의 장기를 초과하는 때에는 금고를 중한\n",
      "것으로 한다.\n",
      "②동종의 형은 장기의 긴 것과 다액의 많은 것을 중한 것으로 하고 장기 또는 다액이 동일한\n",
      "때에는 그 단기의 긴 것과 소액의 많은 것을 중한 것으로 한다.\n",
      "법...\n",
      "--------------------\n",
      "\n",
      "Neo4j driver closed.\n"
     ]
    }
   ],
   "source": [
    "# Example RAG query using vector similarity search\n",
    "def query_graph_rag(driver, query_text, embed_model, top_k=3):\n",
    "    print(f\"\\n--- Querying RAG for: '{query_text}' ---\")\n",
    "    start_time = time.time()\n",
    "\n",
    "    # Embed the query\n",
    "    query_embedding = embed_model.embed_query(query_text)\n",
    "\n",
    "    results = []\n",
    "    with driver.session(database=\"neo4j\") as session:\n",
    "        # Find similar Articles\n",
    "        try:\n",
    "            article_res = session.run(\n",
    "                \"\"\"\n",
    "                CALL db.index.vector.queryNodes('article_embedding', $top_k, $query_embedding) YIELD node, score\n",
    "                RETURN node.id AS article_id, node.text AS text, score\n",
    "                \"\"\",\n",
    "                top_k=top_k,\n",
    "                query_embedding=query_embedding\n",
    "            )\n",
    "            for record in article_res:\n",
    "                 results.append({\n",
    "                     \"type\": \"Article\",\n",
    "                     \"id\": record[\"article_id\"],\n",
    "                     \"score\": record[\"score\"],\n",
    "                     \"text\": record[\"text\"][:300] + \"...\" # Preview\n",
    "                 })\n",
    "        except Exception as e:\n",
    "            print(f\"Could not query Article vector index: {e}\")\n",
    "\n",
    "\n",
    "        # Find similar Precedents\n",
    "        try:\n",
    "            precedent_res = session.run(\n",
    "                \"\"\"\n",
    "                CALL db.index.vector.queryNodes('precedent_embedding', $top_k, $query_embedding) YIELD node, score\n",
    "                MATCH (node)-[:REFERENCES_ARTICLE]->(a:Article)\n",
    "                OPTIONAL MATCH (node)-[:HAS_KEYWORD]->(k:Keyword)\n",
    "                RETURN node.id AS case_id, node.name as case_name, node.full_summary AS text, score,\n",
    "                       collect(DISTINCT a.id) as referenced_articles,\n",
    "                       collect(DISTINCT k.text) as keywords\n",
    "                \"\"\",\n",
    "                top_k=top_k,\n",
    "                query_embedding=query_embedding\n",
    "            )\n",
    "            for record in precedent_res:\n",
    "                 results.append({\n",
    "                     \"type\": \"Precedent\",\n",
    "                     \"id\": record[\"case_id\"],\n",
    "                     \"name\": record[\"case_name\"],\n",
    "                     \"score\": record[\"score\"],\n",
    "                     \"text\": record[\"text\"][:300] + \"...\", # Preview\n",
    "                     \"referenced_articles\": record[\"referenced_articles\"],\n",
    "                     \"keywords\": record[\"keywords\"]\n",
    "                 })\n",
    "        except Exception as e:\n",
    "            print(f\"Could not query Precedent vector index: {e}\")\n",
    "\n",
    "\n",
    "    end_time = time.time()\n",
    "    print(f\"Query completed in {end_time - start_time:.2f} seconds.\")\n",
    "\n",
    "    # Sort results by score (descending) and take top K overall\n",
    "    results.sort(key=lambda x: x[\"score\"], reverse=True)\n",
    "\n",
    "    print(\"\\n--- Top Results ---\")\n",
    "    for i, res in enumerate(results[:top_k]):\n",
    "        print(f\"{i+1}. Type: {res['type']}, ID: {res['id']}, Score: {res['score']:.4f}\")\n",
    "        if res['type'] == 'Precedent':\n",
    "            print(f\"   Name: {res.get('name')}\")\n",
    "            print(f\"   Keywords: {res.get('keywords')}\")\n",
    "            print(f\"   Referenced Articles: {res.get('referenced_articles')}\")\n",
    "        print(f\"   Text Preview: {res['text']}\")\n",
    "        print(\"-\" * 20)\n",
    "\n",
    "    return results[:top_k] # Return top K overall results\n",
    "\n",
    "\n",
    "# Test query\n",
    "query = \"정당방위의 요건은 무엇인가?\"\n",
    "retrieved_context = query_graph_rag(driver, query, embedding_model, top_k=3)\n",
    "\n",
    "# Close driver when completely finished\n",
    "driver.close()\n",
    "print(\"\\nNeo4j driver closed.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28bb9281",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
